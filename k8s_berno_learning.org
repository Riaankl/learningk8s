# Just capturing some ideas

* Lets test functionalit
- Create code block with ,ibs type shell and add your command, 
- To run the command navigate into the block in normal mode and hit ,,
#+begin_src shell
kubectl get svc -A
#+end_src

#+RESULTS:
| NAMESPACE            | NAME                               | TYPE         |     CLUSTER-IP | EXTERNAL-IP | PORT(S)                |  AGE |
| default              | kubernetes                         | ClusterIP    |      10.96.0.1 | <none>      | 443/TCP                |  82d |
| default              | kubia-http                         | LoadBalancer | 10.108.152.157 | <pending>   | 8080:31470/TCP         | 6d8h |
| kube-system          | ingress-nginx-controller-admission | ClusterIP    |  10.99.132.235 | <none>      | 443/TCP                |  82d |
| kube-system          | kube-dns                           | ClusterIP    |     10.96.0.10 | <none>      | 53/UDP,53/TCP,9153/TCP |  82d |
| kube-system          | registry                           | ClusterIP    | 10.100.221.151 | <none>      | 80/TCP,443/TCP         |  82d |
| kubernetes-dashboard | dashboard-metrics-scraper          | ClusterIP    |   10.109.68.77 | <none>      | 8000/TCP               | 6d7h |
| kubernetes-dashboard | kubernetes-dashboard               | ClusterIP    |  10.100.239.36 | <none>      | 80/TCP                 | 6d7h |

ok lets go with the lesson, starting on page 61 3.2 Creating pods from YAML....

#+begin_src shell
kubectl get pods -n default 
#+end_src

#+RESULTS:
| NAME        | READY | STATUS  | RESTARTS | AGE  |
| kubia-4lp4r | 1/1   | Running |        0 | 6d8h |

#+begin_src shell
kubectl get po kubia-4lp4r -o yaml 
#+end_src

I got to introducint the main parts of a pod definition on the bottom or page 62 tbc......

* Pod yaml anatomy
- version of api / kind of resource
- Metadata: Name, namespace, labels and other
- spec: contains the actual description of the pos'd contents, such as the pod's containers, volumes, and other data
- Status: This is only for running container, contains ip, status etc

* lets create a YAML descriptor for a pod
I would like to create the file here and then tangle it out, lets try
#+begin_src yaml :tangle yaml/kubia-manual.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual
spec:
  containers:
  - image: bernokl/kubia
    name: kubia
    ports:
    - containerPort: 8080
      protocol: TCP
#+end_src

Yay tangle ended up being simple, key is header, to generate file hit ,bt
Read more about it on [Babel Intro](https://orgmode.org/worg/org-contrib/babel/intro.html) 
or [examples](https://orgmode.org/manual/Literal-Examples.html)

#+begin_example
  "#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/kubia-manual.yaml"
#+end_example

Lets apply it
#+begin_src shell
kubectl create -f /Users/bernokl/projects/k8sInAction/yaml/kubia-manual.yaml
#+end_src

#+RESULTS:
: pod/kubia-manual created

Lets go look at the new pod
#+begin_src shell
kubectl get pods -n default 
#+end_src

#+RESULTS:
| NAME         | READY | STATUS  | RESTARTS | AGE   |
| kubia-4lp4r  | 1/1   | Running |        0 | 7d8h  |
| kubia-manual | 1/1   | Running |        0 | 3m57s |

Lets look at the logs
#+begin_src shell
 kubectl logs kubia-manual 
#+end_src

#+RESULTS:
: Kubia server starting...

Nice hint if you have multiple containers in a pod you can point to the specific container with -c
#+begin_example
  kubectl logs kubia-manual -c kubia
#+end_example

Note logs die with pods unless centralized logging is configured.

Lets forward the port so we can get to it
(Dont run this, it is interactive and will hang, 
if it does "ps faux | grep port-forward" run in a shell outside emacs should help you locate the pid) 
#+begin_src xx shell
  kubectl port-forward kubia-manual 8888:8080 
#+end_src

The avove is really handy it gives you access from your local network to a pod
#+begin_src shell :results value verbatim
  #curl localhost:8080 
  ps aux | grep port | grep kubia | grep -v grep
#+end_src

#+RESULTS:
: bernokl          54743   0.0  0.1  4445792  28704 s006  S+    7:43AM   0:01.51 kubectl port-forward kubia-manual 8888:8080

* Lets create pods with labels
  
#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/kubia-manual-with-Labels.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: kubia-manual-v2
    labels:
      creation_method: manual
      env: prod
  spec:
    containers:
    - image: bernokl/kubia
      name: kubia
      ports:
      - containerPort: 8080
        protocol: TCP
#+end_src

Tangled  the above out, just for nerdness lets confirm
#+begin_src shell :results value verbatim
head  /Users/bernokl/projects/k8sInAction/yaml/kubia-manual-with-Labels.yaml
#+end_src

#+RESULTS:
#+begin_example
apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual-v2
  labels:
    creation_method: manual
    env: prod
spec:
  containers:
  - image: bernokl/kubia
#+end_example


OK, lets go apply that sucker
#+begin_src shell
kubectl create -f yaml/kubia-manual-with-labels.yaml
#+end_src

#+RESULTS:
: pod/kubia-manual-v2 created

Should we have 3 pods now?
#+begin_src shell :results value verbatim
kubectl get pods -n default 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE
: kubia-4lp4r       1/1     Running   0          7d18h
: kubia-manual      1/1     Running   0          10h
: kubia-manual-v2   1/1     Running   0          76s

Whoop, k lets go look at the labels
#+begin_src shell :results value verbatim
kubectl get po --show-labels -n default 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE     LABELS
: kubia-4lp4r       1/1     Running   0          7d18h   app=kubia
: kubia-manual      1/1     Running   0          10h     <none>
: kubia-manual-v2   1/1     Running   0          2m28s   creation_method=manual,env=prod

It is also possibel to organize the layout for label values
#+begin_src shell :results value verbatim
kubectl get pods -L creation_method,env 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE     CREATION_METHOD   ENV
: kubia-4lp4r       1/1     Running   0          7d19h                     
: kubia-manual      1/1     Running   0          10h                       
: kubia-manual-v2   1/1     Running   0          5m29s   manual            prod

Lets go update labels of our running pod
#+begin_src shell :results value verbatim
kubectl label po kubia-manual creation_method=manual 
#+end_src

#+RESULTS:
: pod/kubia-manual labeled

ok lets go see what that label looks like
#+begin_src shell :results value verbatim
kubectl get pods -L creation_method,env 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE     CREATION_METHOD   ENV
: kubia-4lp4r       1/1     Running   0          7d19h                     
: kubia-manual      1/1     Running   0          10h     manual            
: kubia-manual-v2   1/1     Running   0          24s     manual            prod

Nice!

Lets go overwrite an existing label say change env to prod for v2
#+begin_src shell :results value verbatim
kubectl label po kubia-manual-v2 env=debug --overwrite 
#+end_src

#+RESULTS:
: pod/kubia-manual-v2 labeled

ok lets go see what that label looks like
#+begin_src shell :results value verbatim
kubectl get pods -L creation_method,env 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE     CREATION_METHOD   ENV
: kubia-4lp4r       1/1     Running   0          7d19h                     
: kubia-manual      1/1     Running   0          10h     manual            
: kubia-manual-v2   1/1     Running   0          45s     manual            debug

More playing with labels
Lets just list pods that were manually created
#+begin_src shell :results value verbatim
kubectl get po -l creation_method=manual 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE
: kubia-manual      1/1     Running   0          10h
: kubia-manual-v2   1/1     Running   0          9m37s

Lets get all pods with an env labels
#+begin_src shell :results value verbatim
kubectl get po -l env 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE
: kubia-manual-v2   1/1     Running   0          10m

Lets get all pods without env labels
#+begin_src shell :results value verbatim
kubectl get po -l '!env',creation_method!=manual
#+end_src

#+RESULTS:
: NAME          READY   STATUS    RESTARTS   AGE
: kubia-4lp4r   1/1     Running   0          7d19h

That is pretty sweet I added a second condition that just worked looks like it could be very powerful

One last thing lets label our node so we can organize by those labels
#+begin_src shell :results value verbatim 
  kubectl get node
  kubectl label node minikube fun=true
  kubectl get nodes -l fun=true
  kubectl get nodes -L fun=true
#+end_src

#+RESULTS:
: NAME       STATUS   ROLES    AGE   VERSION
: minikube   Ready    master   83d   v1.18.3

: NAME       STATUS   ROLES    AGE   VERSION
: minikube   Ready    master   83d   v1.18.3

: NAME       STATUS   ROLES    AGE   VERSION   FUN=TRUE
: minikube   Ready    master   83d   v1.18.3   

* Schedule pods to specific nodes

#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/kubia-fun.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: kubia-fun
  spec:
    nodeSelector:
      fun: "true"
    containers:
    - image: bernokl/kubia
      name: kubia
#+end_src

lets go apply it, this will only allow my kibia fun pod to run on a node that has fun as true
In my case I am using minikube so I only have one node, but the idea is amazing
Note that it is nodeSelector above that ties my pod to that node
#+begin_src shell  
kubectl create -f yaml/kubia-fun.yaml
#+end_src

#+RESULTS:
: pod/kubia-fun created

#+begin_src shell :results value verbatim 
kubectl get nodes --show-labels
#+end_src 

#+RESULTS:
: NAME       STATUS   ROLES    AGE   VERSION   LABELS
: minikube   Ready    master   83d   v1.18.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,fun=true,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,minikube.k8s.io/commit=2243b4b97c131e3244c5f014faedca0d846599f5,minikube.k8s.io/name=minikube,minikube.k8s.io/updated_at=2020_08_20T10_14_48_0700,minikube.k8s.io/version =v1.12.3,node-role.kubernetes.io/master=


#+begin_src bash
kubectl get po -n default
#+end_src

#+RESULTS:
| NAME            | READY | STATUS  | RESTARTS | AGE   |
| kubia-4lp4r     | 1/1   | Running |        0 | 7d20h |
| kubia-fun       | 1/1   | Running |        0 | 23m   |
| kubia-manual    | 1/1   | Running |        0 | 11h   |
| kubia-manual-v2 | 1/1   | Running |        0 | 61m   |

 
* Anotations
  Very handy, we can use them to add metadata, they are not meant to be used to sort on like labels, but rather provide info
#+begin_src shell :results value verbatim 
  kubectl get po kubia-manual -o yaml > tmp
  head tmp
  rm tmp
#+end_src

#+RESULTS:
#+begin_example
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2020-11-11T09:54:20Z"
  labels:
    creation_method: manual
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
#+end_example

Ok the guy above has no annotations, lets add one
#+begin_src shell :results value verbatim
kubectl annotate pod kubia-manual bernokl.com/letsaddanotation="interesting facts" 
#+end_src

#+RESULTS:
: pod/kubia-manual annotated


#+begin_src shell :results value verbatim 
  kubectl get po kubia-manual -o yaml > tmp
  head tmp
  rm tmp
#+end_src

#+RESULTS:
#+begin_example
apiVersion: v1
kind: Pod
metadata:
  annotations:
    bernokl.com/letsaddanotation: interesting facts
  creationTimestamp: "2020-11-11T09:54:20Z"
  labels:
    creation_method: manual
  managedFields:
  - apiVersion: v1
#+end_example
 
Lets describe it
#+begin_src shell :results verbatim 
kubectl describe pod kubia-manual
#+end_src

#+RESULTS:
#+begin_example
Name:         kubia-manual
Namespace:    default
Priority:     0
Node:         minikube/192.168.64.2
Start Time:   Wed, 11 Nov 2020 22:54:20 +1300
Labels:       creation_method=manual
Annotations:  bernokl.com/letsaddanotation: interesting facts
Status:       Running
IP:           172.17.0.9
IPs:
  IP:  172.17.0.9
Containers:
  kubia:
    Container ID:   docker://f2febbb9016aeb839478ce1869b88dfa8228d3b4ec5edf646898bd5d9fb21d2a
    Image:          bernokl/kubia
    Image ID:       docker-pullable://bernokl/kubia@sha256:13f94084a4515abf331b5a9e751e355964624124c52755a6589abde31afd5c64
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 11 Nov 2020 22:54:27 +1300
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xnktg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-xnktg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-xnktg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>
#+end_example


* Name spaces
Lets see what namespaces we have
#+begin_src shell :results verbatim
kubectl get ns 
#+end_src

#+RESULTS:
: NAME                   STATUS   AGE
: default                Active   84d
: kube-node-lease        Active   84d
: kube-public            Active   84d
: kube-system            Active   84d
: kubernetes-dashboard   Active   7d20h

Lets look to see what we can find in kube-system
#+begin_src shell :results verbatim 
kubectl get po --namespace kube-system
#+end_src

#+RESULTS:
#+begin_example
NAME                                        READY   STATUS      RESTARTS   AGE
coredns-66bff467f8-96gv4                    1/1     Running     1          84d
etcd-minikube                               1/1     Running     0          8d
ingress-nginx-admission-create-fpcvv        0/1     Completed   0          84d
ingress-nginx-admission-patch-6qfcz         0/1     Completed   0          84d
ingress-nginx-controller-799dcd5d57-cmd68   1/1     Running     0          8d
kube-apiserver-minikube                     1/1     Running     1          84d
kube-controller-manager-minikube            1/1     Running     1          84d
kube-proxy-d892r                            1/1     Running     1          84d
kube-scheduler-minikube                     1/1     Running     1          84d
registry-65xp4                              1/1     Running     1          84d
registry-proxy-dqn2b                        1/1     Running     1          84d
storage-provisioner                         1/1     Running     5          84d
#+end_example

Lets create a new namespace using a yaml file
#+begin_src yaml :tangle yaml/custom-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: custom-namespace
#+end_src

Two easy ways to create it, run the file above or even better just run `kubectl create namespace`
#+begin_src shell :results raw
  #kubectl create -f yaml/custom-namespace.yaml
  kubectl delete namespace custom-namespace2
#+end_src

#+RESULTS:
namespace "custom-namespace2" deleted
namespace/custom-namespace2 created
namespace/custom-namespace created

NO DOTS IN NAMESPACE NAMES

To create someting in ns simply add namespace: to the metadata of the pod.yaml or identify it in line
#+begin_src shell :results raw
kubectl create -f yaml/kubia-manual.yaml -n custom-namespace 
#+end_src

#+RESULTS:
pod/kubia-manual created

#+begin_src shell :results raw
kubectl get pods -A
#+end_src

#+RESULTS:
NAMESPACE              NAME                                        READY   STATUS      RESTARTS   AGE
custom-namespace       kubia-manual                                1/1     Running     0          2m11s
default                kubia-4lp4r                                 1/1     Running     0          7d21h
default                kubia-fun                                   1/1     Running     0          125m
default                kubia-manual                                1/1     Running     0          13h
default                kubia-manual-v2                             1/1     Running     0          163m
kube-system            coredns-66bff467f8-96gv4                    1/1     Running     1          84d
kube-system            etcd-minikube                               1/1     Running     0          8d
kube-system            ingress-nginx-admission-create-fpcvv        0/1     Completed   0          84d
etc.......

#+begin_src shell
kubectl get pods 
#+end_src

#+RESULTS:
| NAME            | READY | STATUS  | RESTARTS | AGE   |
| kubia-4lp4r     | 1/1   | Running |        0 | 7d22h |
| kubia-fun       | 1/1   | Running |        0 | 135m  |
| kubia-manual    | 1/1   | Running |        0 | 13h   |
| kubia-manual-v2 | 1/1   | Running |        0 | 173m  |

Looks like my ns if I do not declare it is set to default, lets try to change it to custom-namespace
#+begin_src shell 
#kubectl config set-context --current --namespace=custom-namespace
kubectl config set-context --current --namespace=default
#+end_src

#+RESULTS:
: minikube

#+begin_src shell
kubectl get pods 
#+end_src

#+RESULTS:
| NAME         | READY | STATUS  | RESTARTS | AGE |
| kubia-manual | 1/1   | Running |        0 | 16m |

yay, makes sense, easy isolation for resources.

ok lets clean up
#+begin_src shell
kubectl delete po kubia-4lp4r
#+end_src

#+RESULTS:
: kubia-4lp4r

Lets delete by labels
#+begin_src shell
kubectl delete po -l creation_method=manual 
#+end_src

#+RESULTS:
| pod | kubia-manual    | deleted |
| pod | kubia-manual-v2 | deleted |

Lets delete the entire namespace
#+begin_src shell
kubectl delete ns custom-namespace 
#+end_src

#+RESULTS:
: custom-namespace

OK at this point I should just have the one pod remaining in default, lets confirm
#+begin_src shell
kubectl get po -A 
#+end_src

#+RESULTS:
| NAMESPACE            | NAME                                      | READY | STATUS    | RESTARTS |   AGE |
| default              | kubia-8hvvl                               | 1/1   | Running   |        0 | 5m10s |
| default              | kubia-fun                                 | 1/1   | Running   |        0 |  151m |
| kube-system          | coredns-66bff467f8-96gv4                  | 1/1   | Running   |        1 |   84d |
| kube-system          | etcd-minikube                             | 1/1   | Running   |        0 |    8d |
etc......

ok forgot about the one we tested nodeSelector on, k lets delete all pods in default see how we go
 #+begin_src shell
kubectl delete po --all -ndefault 
 #+end_src

 #+RESULTS:
 | pod | kubia-8hvvl | deleted |
 | pod | kubia-fun   | deleted |

 Lets confirm
#+begin_src shell
kubectl get po -A 
#+end_src

#+RESULTS:
| NAMESPACE            | NAME                                      | READY | STATUS    | RESTARTS |   AGE |
| default              | kubia-cj54r                               | 1/1   | Running   |        0 |   59s |
| kube-system          | coredns-66bff467f8-96gv4                  | 1/1   | Running   |        1 |   84d |
etc.....

O wait, kubia-cj54r is back, that is right it is a service so even if I delete the pod it will be back
This command very much seems like rm -rf to me, but you can delete most everything in a ns in one neat command
#+begin_src shell
kubectl delete all --all -n default
#+end_src

#+RESULTS:
| pod                   | kubia-cj54r | deleted |
| replicationcontroller | kubia       | deleted |
| service               | kubernetes  | deleted |
| service               | kubia-http  | deleted |

Nice it cleaned up replication controller and services I forgot about fs

* Pod health LIVENESS PROBES
3 types of livliness probes
- http get, performs GET request
- TCP socket probe, tries to bind to the specified port of the container
- Exec, executes an arbitrary command inside the container

To test http livliness probe use an image that has the nodjsApp report 500 after every 5th reques 
Lets build the yaml to create this pod

#+begin_src yaml :tangle yaml/kubia-liveness-probe.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: kubia-liveness
  spec:
    containers:
    - image: luksa/kubia-unhealthy
      name: kubia
      livenessProbe:
        httpGet:
          path: /
          port: 8080
#+end_src

lets go apply it
#+begin_src shell :results raw
kubectl apply -f yaml/kubia-liveness-probe.yaml 
#+end_src

#+RESULTS:
pod/kubia-liveness created

#+begin_src shell results:raw
kubectl get po kubia-liveness
#+end_src

#+RESULTS:
| NAME           | READY | STATUS  | RESTARTS | AGE   |
| kubia-liveness | 1/1   | Running |        2 | 4m44s |

You can look at the logs for the pod that already crashed by using `--previous`
#+begin_src shell
kubectl logs kubia-liveness --previous 
#+end_src

#+RESULTS:
| Kubia    | server  | starting... |                   |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |

I still think the best info on a container is in describe, Events is full of gold
#+begin_src shell :results raw
  kubectl describe po kubia-liveness > tmp
  head -15 tmp
  echo "----------------------------------"
  grep -i -b2 -a2 "last state" tmp
  echo "----------------------------------"
  grep -i -b2 -a2 "liveness" tmp
  echo "----------------------------------"
  tail -10 tmp
  rm tmp 
#+end_src

#+RESULTS:
Name:         kubia-liveness
Namespace:    default
Priority:     0
Node:         minikube/192.168.64.2
Start Time:   Thu, 12 Nov 2020 15:24:43 +1300
Labels:       <none>
Annotations:  Status:  Running
IP:           172.17.0.6
IPs:
  IP:  172.17.0.6
Containers:
  kubia:
    Container ID:   docker://d130e21b7472b362d744e5f8ec80439bcddecd435040ef3312477db2299f81a9
    Image:          luksa/kubia-unhealthy
    Image ID:       docker-pullable://luksa/kubia-unhealthy@sha256:5c746a42612be61209417d913030d97555cff0b8225092908c57634ad7c235f7
----------------------------------
592-    State:          Waiting
620-      Reason:       CrashLoopBackOff
657:    Last State:     Terminated
688-      Reason:       Error
714-      Exit Code:    137
----------------------------------
0:Name:         kubia-liveness
29-Namespace:    default
51-Priority:     0
--
--
842-    Ready:          False
868-    Restart Count:  9
890:    Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
986-    Environment:    <none>
1013-    Mounts:
--
--
1593-  Type     Reason     Age                   From               Message
1664-  ----     ------     ----                  ----               -------
1735:  Normal   Scheduled  27m                   default-scheduler  Successfully assigned default/kubia-liveness to minikube
1855-  Normal   Created    23m (x3 over 26m)     kubelet, minikube  Created container kubia
1942-  Normal   Started    23m (x3 over 26m)     kubelet, minikube  Started container kubia
--
--
1855-  Normal   Created    23m (x3 over 26m)     kubelet, minikube  Created container kubia
1942-  Normal   Started    23m (x3 over 26m)     kubelet, minikube  Started container kubia
2029:  Normal   Killing    21m (x3 over 25m)     kubelet, minikube  Container kubia failed liveness probe, will be restarted
2149-  Normal   Pulling    21m (x4 over 27m)     kubelet, minikube  Pulling image "luksa/kubia-unhealthy"
2250-  Normal   Pulled     21m (x4 over 26m)     kubelet, minikube  Successfully pulled image "luksa/kubia-unhealthy"
--
--
2250-  Normal   Pulled     21m (x4 over 26m)     kubelet, minikube  Successfully pulled image "luksa/kubia-unhealthy"
2363-  Warning  BackOff    6m59s (x27 over 14m)  kubelet, minikube  Back-off restarting failed container
2463:  Warning  Unhealthy  85s (x28 over 25m)    kubelet, minikube  Liveness probe failed: HTTP probe failed with statuscode: 500
----------------------------------
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  27m                   default-scheduler  Successfully assigned default/kubia-liveness to minikube
  Normal   Created    23m (x3 over 26m)     kubelet, minikube  Created container kubia
  Normal   Started    23m (x3 over 26m)     kubelet, minikube  Started container kubia
  Normal   Killing    21m (x3 over 25m)     kubelet, minikube  Container kubia failed liveness probe, will be restarted
  Normal   Pulling    21m (x4 over 27m)     kubelet, minikube  Pulling image "luksa/kubia-unhealthy"
  Normal   Pulled     21m (x4 over 26m)     kubelet, minikube  Successfully pulled image "luksa/kubia-unhealthy"
  Warning  BackOff    6m59s (x27 over 14m)  kubelet, minikube  Back-off restarting failed container
  Warning  Unhealthy  85s (x28 over 25m)    kubelet, minikube  Liveness probe failed: HTTP probe failed with statuscode: 500


From the liveness heading above we see that the delay is set to 0s
That means it starts probing as soon as the new container gets created
it is a good idea to add `initialDelaySeconds: 15` so the probe waits for the container to get ready before hitting

Exit code 137 means pod was terminated by external process ie k8s, it failed the livenessProbe and was killed

Make sure liveness only checks things internal to the pod, 
a front end pod's liveness probe should not report failed if the db goes away

Keep the probe light weight, do not use a call that is a lot of compute or time

A pitfall of livenessProbe is Java, do not use exec probe as it will have to spin up jvm every time


* ReplicationControllers
  3 Parts of a replicationControler
- Label selector (seems to be interchangeable with pod selector?)
- Replica count
- Pod template

Lets go create one so we can look at the anatomy

#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/kubia-rc.yaml
  apiVersion: v1
  kind: ReplicationController
  metadata:
    name: kubia
  spec:
    replicas: 3
    selector:
      app: kubia
    template:
      metadata:
        labels:
          app: kubia
      spec:
        containers: 
        - name: kubia
          image: bernokl/kubia
          ports:
            - containerPort: 8080
    #+end_src

Tangled out the yaml with my trusty ,bt it still makes me happy, what can I say
Ok lets go apply the new yaml and see if we get 3 new instances of kubia running
TIP: if you do not define the selector in rc spec it will be derived from pod template
no need to add it.

ok
Its been a while lets see what pods are running in default
#+begin_src shell :results raw
kubectl get pods 
#+end_src

#+RESULTS:
NAME             READY   STATUS             RESTARTS   AGE
kubia-liveness   0/1     CrashLoopBackOff   177        20h

Good so the liveness test is still going, will leave it for now
Lets go apply our yaml and see if we conjured a new rc for kubia
#+begin_src shell :results raw
kubectl create -f yaml/kubia-rc.yaml 
#+end_src

#+RESULTS:
replicationcontroller/kubia created

Lets check what we did
#+begin_src shell :results raw
kubectl get pods 
#+end_src

#+RESULTS:
NAME             READY   STATUS             RESTARTS   AGE
kubia-liveness   0/1     CrashLoopBackOff   179        20h
kubia-pqwfg      1/1     Running            0          15s
kubia-qcjmd      1/1     Running            0          15s
kubia-v5zpr      1/1     Running            0          15s

O nice, I have 3 running pods, lets go off the reservation and kill one of them, see what happens

#+begin_src shell :results raw
kubectl delete po kubia-pqwfg
#+end_src

#+RESULTS:
pod "kubia-pqwfg" deleted


#+begin_src shell :results raw
kubectl get pods 
#+end_src

#+RESULTS:
NAME             READY   STATUS    RESTARTS   AGE
kubia-f6jv5      1/1     Running   0          59s
kubia-liveness   1/1     Running   182        21h
kubia-qcjmd      1/1     Running   0          10m
kubia-v5zpr      1/1     Running   0          10m

O snap, that container got replaced by a new one, go replicationControler go
Lets see what info we can get on it


#+begin_src shell :results raw
kubectl get rc
#+end_src

#+RESULTS:
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       13m

Lets see if we can look at the description

#+begin_src shell :results raw
kubectl describe rc kubia
#+end_src

#+RESULTS:
Name:         kubia
Namespace:    default
Selector:     app=kubia
Labels:       app=kubia
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        bernokl/kubia
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From                    Message
  ----    ------            ----   ----                    -------
  Normal  SuccessfulCreate  14m    replication-controller  Created pod: kubia-pqwfg
  Normal  SuccessfulCreate  14m    replication-controller  Created pod: kubia-qcjmd
  Normal  SuccessfulCreate  14m    replication-controller  Created pod: kubia-v5zpr
  Normal  SuccessfulCreate  4m52s  replication-controller  Created pod: kubia-f6jv5

 
This is cool, we are going to change the labels on pods to show that there is no relationship
between the pod and the rc besides the label, as soon as we change the label of one of our pods
my bet is that a new pod will be created because the kubia label count will be one short
#+begin_src shell :results raw
kubectl get pods
#+end_src

#+RESULTS:
NAME             READY   STATUS             RESTARTS   AGE
kubia-f6jv5      1/1     Running            0          15m
kubia-liveness   0/1     CrashLoopBackOff   185        21h
kubia-qcjmd      1/1     Running            0          24m
kubia-v5zpr      1/1     Running            0          24m


#+begin_src shell :results raw
kubectl label pod kubia-f6jv5 type=special
#+end_src

#+RESULTS:
pod/kubia-f6jv5 labeled


#+begin_src shell :results raw
kubectl get pods --show-labels
#+end_src

#+RESULTS:
NAME             READY   STATUS             RESTARTS   AGE   LABELS
kubia-f6jv5      1/1     Running            0          16m   app=kubia,type=special
kubia-liveness   0/1     CrashLoopBackOff   185        21h   <none>
kubia-qcjmd      1/1     Running            0          25m   app=kubia
kubia-v5zpr      1/1     Running            0          25m   app=kubia

You can see the rc does not care about additional labels

Now lets overwrite app on that on
#+begin_src shell :results raw
kubectl label pod kubia-f6jv5 app=foo --overwrite
#+end_src

#+RESULTS:
pod/kubia-f6jv5 labeled


Lets go see
#+begin_src shell :results raw
kubectl get pods -L app
#+end_src

#+RESULTS:
NAME             READY   STATUS              RESTARTS   AGE   APP
kubia-f6jv5      1/1     Running             0          20m   foo
kubia-liveness   1/1     Running             187        21h   
kubia-qcjmd      1/1     Running             0          29m   kubia
kubia-v5zpr      1/1     Running             0          29m   kubia
kubia-wbpzv      0/1     ContainerCreating   0          3s    kubia

Ahh snap, new container coming up because the count of kubia was one short

k lets clean up a bit going to delete foo and liveness test
#+begin_src shell :results raw
kubectl delete pod kubia-f6jv5
#+end_src

#+RESULTS:
pod "kubia-f6jv5" deleted
pod "kubia-liveness" deleted

O the next plan is to directly edit rc to add an additional label to the pod template,
wonder what will happen here, my bet is we will hang the editor
#+begin_src shell : results raw
kubectl edit rc kubia 
#+end_src

#+RESULTS:

It did not hang, but rather errored out trying to open, vi do I want to set my editor to emacs? mmm lets hold off, gonna go run that in the terminal, back in a jiff
k edited the rc to add a second label to pod template, lets see what I did

#+begin_src shell :results raw
kubectl get pods -L app
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE   APP
kubia-qcjmd   1/1     Running   0          53m   kubia
kubia-v5zpr   1/1     Running   0          53m   kubia
kubia-wbpzv   1/1     Running   0          23m   kubia

k no change, lets delete one

#+begin_src shell :results raw
kubectl delete pod kubia-qcjmd
#+end_src

#+RESULTS:
pod "kubia-qcjmd" deleted


#+begin_src shell :results raw
kubectl get pods --show-labels
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE     LABELS
kubia-98ppm   1/1     Running   0          2m38s   app=kubia,bk=whoot
kubia-v5zpr   1/1     Running   0          57m     app=kubia
kubia-wbpzv   1/1     Running   0          27m     app=kubia

Lets go back to the editor bit, I see there is kube_editor, let me try to set it to emacs, wee what happens
#+begin_src shell :results raw
which emacs 
#+end_src

#+RESULTS:
/usr/local/bin/emacs

#+begin_src shell :results raw
  set KUBE_EDITOR emacs
#+end_src

#+RESULTS:

Lets try and edit again
#+begin_src shell : results raw
kubectl edit rc kubia 
#+end_src

#+RESULTS:

Nope still falls back to vi, there is a really nice kubernetes layer built in so going to leave this for now

Back to ReplicationControllers, lets go add some replicas, see what we seems


#+begin_src shell : results raw
kubectl scale rc kubia --replicas=10 
#+end_src

#+RESULTS:
: replicationcontroller/kubia scaled

#+begin_src shell : results raw
kubectl get pods
#+end_src

#+RESULTS:
| NAME        | READY | STATUS            | RESTARTS | AGE |
| kubia-9482q | 1/1   | Running           |        0 | 23s |
| kubia-98ppm | 1/1   | Running           |        0 | 13m |
| kubia-9xc8x | 0/1   | ContainerCreating |        0 | 23s |
| kubia-cbqmm | 1/1   | Running           |        0 | 23s |
| kubia-nhp94 | 1/1   | Running           |        0 | 23s |
| kubia-pfsdq | 1/1   | Running           |        0 | 23s |
| kubia-v4ss5 | 1/1   | Running           |        0 | 23s |
| kubia-v5zpr | 1/1   | Running           |        0 | 68m |
| kubia-wbpzv | 1/1   | Running           |        0 | 39m |
| kubia-wk77p | 1/1   | Running           |        0 | 23s |

That is badass


#+begin_src shell : results raw
kubectl scale rc kubia --replicas=3 
#+end_src

#+RESULTS:
: replicationcontroller/kubia scaled

#+begin_src shell : results raw
kubectl get pods
#+end_src

#+RESULTS:
| NAME        | READY | STATUS      | RESTARTS | AGE |
| kubia-9482q | 1/1   | Terminating |        0 | 84s |
| kubia-98ppm | 1/1   | Running     |        0 | 14m |
| kubia-9xc8x | 1/1   | Terminating |        0 | 84s |
| kubia-cbqmm | 1/1   | Terminating |        0 | 84s |
| kubia-nhp94 | 1/1   | Terminating |        0 | 84s |
| kubia-pfsdq | 1/1   | Terminating |        0 | 84s |
| kubia-v4ss5 | 1/1   | Terminating |        0 | 84s |
| kubia-v5zpr | 1/1   | Running     |        0 | 69m |
| kubia-wbpzv | 1/1   | Running     |        0 | 40m |
| kubia-wk77p | 1/1   | Terminating |        0 | 84s |

So cool so damn trivial. One very important thing, the scale command, edits rc kubia and changes the `replicas:`
We are not telling k8s to change the pods, we are updating our declared state and k8s adjust reality to fit that state
Neat huh?

When rc's get deleted they typically clean up the pods, but because there is nothing your pods need from the
rc you can delete it without deleteing the pods simply use
#+begin_src shell :results raw
kubectl delete rc kubia --cascade=false
#+end_src

#+RESULTS:

OK, it seems replicationControlers are being replaced with ReplicaSets, you do not directly create them as a rule
they are usually created by higher level processes like deploys. ReplicaSets can do a few things replicationControlers
can not, for instance they can accept multiple labels, they can also use label key without value, so it would allow you
to run env=prod, env=dev etc as labels: env

Lets go manually make a replicaSet


#+begin_src yaml :tangle yaml/kubia-replicaset.yaml
  apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    name: kubia
  spec: 
    replicas: 3
    selector:
      matchLabels:
        app: kubia
    template:
      metadata:
        labels:
          app: kubia
      spec:
        containers:
        - name: kubia
          image: bernokl/kubia
          ports:
            - containerPort: 8080
#+end_src

#+RESULTS:

#+begin_src shell :results raw
kubectl get pods
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE
kubia-rf7gz   1/1     Running   0          4d6h
kubia-v5zpr   1/1     Running   0          4d9h
kubia-wbpzv   1/1     Running   0          4d9h
NAME          READY   STATUS    RESTARTS   AGE
kubia-98ppm   1/1     Running   0          111m
kubia-v5zpr   1/1     Running   0          166m
kubia-wbpzv   1/1     Running   0          137m

#+begin_src shell :results raw
kubectl delete -f yaml/kubia-replicaset.yaml
#+end_src

#+RESULTS:
replicaset.apps "kubia" deleted
replicaset.apps/kubia created
replicaset.apps "kubia" deleted
replicaset.apps/kubia created


#+begin_src shell :results raw
  kubectl get replicaSet -A
#+end_src

#+RESULTS:
NAMESPACE              NAME                                  DESIRED   CURRENT   READY   AGE
default                kubia                                 3         3         3       18s
kube-system            coredns-66bff467f8                    1         1         1       89d
kube-system            ingress-nginx-controller-69ccf5d9d8   0         0         0       89d
kube-system            ingress-nginx-controller-799dcd5d57   1         1         1       13d
kubernetes-dashboard   dashboard-metrics-scraper-dc6947fbf   1         1         1       13d
kubernetes-dashboard   kubernetes-dashboard-58b79879c5       1         1         1       13d
NAMESPACE              NAME                                  DESIRED   CURRENT   READY   AGE
default                kubia                                 3         3         3       42m
kube-system            coredns-66bff467f8                    1         1         1       85d
kube-system            ingress-nginx-controller-69ccf5d9d8   0         0         0       85d
kube-system            ingress-nginx-controller-799dcd5d57   1         1         1       9d
kubernetes-dashboard   dashboard-metrics-scraper-dc6947fbf   1         1         1       9d
kubernetes-dashboard   kubernetes-dashboard-58b79879c5       1         1         1       9d

#+begin_src shell :results raw
kubectl describe rs
#+end_src

#+RESULTS:
Name:         kubia
Namespace:    default
Selector:     app=kubia
Labels:       <none>
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        bernokl/kubia
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  40s   replicaset-controller  Created pod: kubia-jbnb6
  Normal  SuccessfulCreate  40s   replicaset-controller  Created pod: kubia-7ljzb
  Normal  SuccessfulCreate  40s   replicaset-controller  Created pod: kubia-74fkq


Biggest improvement for rs over rc is the matchExpressions, it is very powerful

#+begin_src shell
kubectl get pods -n default 
#+end_src

#+RESULTS:
| NAME        | READY | STATUS  | RESTARTS | AGE   |
| kubia-74fkq | 1/1   | Running |        0 | 3m46s |
| kubia-7ljzb | 1/1   | Running |        0 | 3m46s |
| kubia-jbnb6 | 1/1   | Running |        0 | 3m46s |


#+begin_src yaml :tangle yaml/kubia-replicaset-matchexpressions.yaml
  apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    name: kubia
  spec: 
    replicas: 3
    selector:
      matchExpressions:
        - key: app
          operator: In
          values:
            - kubia
    template:
      metadata:
        labels:
          app: kubia
      spec:
        containers:
        - name: kubia
          image: bernokl/kubia
          ports:
            - containerPort: 8080
#+end_src

#+begin_src shell :results raw
kubectl create -f yaml/kubia-replicaset-matchExpressions.yaml 
#+end_src

#+RESULTS:
replicaset.apps/kubia created

#+begin_src shell :results raw
kubectl describe rs  
#+end_src

#+RESULTS:
Name:         kubia
Namespace:    default
Selector:     app in (kubia)
Labels:       <none>
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        bernokl/kubia
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From                   Message
  ----    ------            ----   ----                   -------
  Normal  SuccessfulCreate  3m47s  replicaset-controller  Created pod: kubia-6rts5
  Normal  SuccessfulCreate  3m47s  replicaset-controller  Created pod: kubia-v96d4
  Normal  SuccessfulCreate  3m47s  replicaset-controller  Created pod: kubia-c5mfw
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       3m30s

- In—Label’s value must match one of the specified values.
- NotIn—Label’s value must not match any of the specified values.
- Exists—Pod must include a label with the specified key (the value isn’t import-
ant). When using this operator, you shouldn’t specify the values field.
- DoesNotExist—Pod must not include a label with the specified key. The values
property must not be specified.

Lets clean up
#+begin_src shell :results raw
kubectl delete rs kubia 
#+end_src

#+RESULTS:
replicaset.apps "kubia" deleted

#+begin_src shell :results raw
kubectl get po -A 
#+end_src

#+RESULTS:
NAMESPACE              NAME                                        READY   STATUS      RESTARTS   AGE
kube-system            coredns-66bff467f8-96gv4                    1/1     Running     1          89d
kube-system            etcd-minikube                               1/1     Running     0          13d
kube-system            ingress-nginx-admission-create-fpcvv        0/1     Completed   0          89d
kube-system            ingress-nginx-admission-patch-6qfcz         0/1     Completed   0          89d
kube-system            ingress-nginx-controller-799dcd5d57-cmd68   1/1     Running     0          13d
kube-system            kube-apiserver-minikube                     1/1     Running     1          89d
kube-system            kube-controller-manager-minikube            1/1     Running     1          89d
kube-system            kube-proxy-d892r                            1/1     Running     1          89d
kube-system            kube-scheduler-minikube                     1/1     Running     1          89d
kube-system            registry-65xp4                              1/1     Running     1          89d
kube-system            registry-proxy-dqn2b                        1/1     Running     1          89d
kube-system            storage-provisioner                         1/1     Running     5          89d
kubernetes-dashboard   dashboard-metrics-scraper-dc6947fbf-bp98k   1/1     Running     0          13d
kubernetes-dashboard   kubernetes-dashboard-58b79879c5-2w2wl       1/1     Running     0          13d

* DaemonSet
- This is a resource that is supposed to run one pod on each node
- examples is sytem level resources you want on each node, logging, monitoring etc.
- It is not controlled by the scheduler, will add pods to nodes with scheduling off
- You can add a nodeSelector that will allow daemonset to run on a subset of nodes.

#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/ssd-monitor-daemonset.yaml
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    name: ssd-monitor
  spec:
    selector:
      matchLabels:
        app: ssd-monitor
    template:
      metadata:
        labels:
          app: ssd-monitor
      spec:
        nodeSelector:
          disk: ssd
        containers:
        - name: main
          image: luksa/ssd-monitor
#+end_src


Ok lets apply that sucker
#+begin_src shell :results raw
kubectl apply -f yaml/ssd-monitor-daemonset.yaml 
#+end_src

#+RESULTS:
daemonset.apps/ssd-monitor created

#+begin_src shell :results raw
kubectl get ds
#+end_src

#+RESULTS:
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   0         0         0       0            0           disk=ssd        29s

See no pods running, because none of the nodes have a label disk=ssd, lets look
#+begin_src shell :results raw
kubectl get node 
#+end_src

#+RESULTS:
NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   89d   v1.18.3

Lets go label it, wait before we do that lets jsut confirm the pods we see

#+begin_src shell :results raw
kubectl get pods -A
#+end_src

#+RESULTS:
NAMESPACE              NAME                                        READY   STATUS      RESTARTS   AGE
kube-system            coredns-66bff467f8-96gv4                    1/1     Running     1          89d
kube-system            etcd-minikube                               1/1     Running     0          13d
kube-system            ingress-nginx-admission-create-fpcvv        0/1     Completed   0          89d
kube-system            ingress-nginx-admission-patch-6qfcz         0/1     Completed   0          89d
kube-system            ingress-nginx-controller-799dcd5d57-cmd68   1/1     Running     0          13d
kube-system            kube-apiserver-minikube                     1/1     Running     1          89d
kube-system            kube-controller-manager-minikube            1/1     Running     1          89d
kube-system            kube-proxy-d892r                            1/1     Running     1          89d
kube-system            kube-scheduler-minikube                     1/1     Running     1          89d
kube-system            registry-65xp4                              1/1     Running     1          89d
kube-system            registry-proxy-dqn2b                        1/1     Running     1          89d
kube-system            storage-provisioner                         1/1     Running     5          89d
kubernetes-dashboard   dashboard-metrics-scraper-dc6947fbf-bp98k   1/1     Running     0          13d
kubernetes-dashboard   kubernetes-dashboard-58b79879c5-2w2wl       1/1     Running     0          13d

#+begin_src shell :results raw
kubectl label node minikube disk=ssd 
#+end_src

#+RESULTS:
node/minikube labeled

#+begin_src shell :results raw
kubectl get pods
#+end_src

#+RESULTS:
NAME                READY   STATUS    RESTARTS   AGE
ssd-monitor-vgqwv   1/1     Running   0          6m42s
NAMESPACE              NAME                                        READY   STATUS      RESTARTS   AGE
default                ssd-monitor-vgqwv                           1/1     Running     0          17s
kube-system            coredns-66bff467f8-96gv4                    1/1     Running     1          89d
kube-system            etcd-minikube                               1/1     Running     0          13d
kube-system            ingress-nginx-admission-create-fpcvv        0/1     Completed   0          89d
kube-system            ingress-nginx-admission-patch-6qfcz         0/1     Completed   0          89d
kube-system            ingress-nginx-controller-799dcd5d57-cmd68   1/1     Running     0          13d
kube-system            kube-apiserver-minikube                     1/1     Running     1          89d
kube-system            kube-controller-manager-minikube            1/1     Running     1          89d
kube-system            kube-proxy-d892r                            1/1     Running     1          89d
kube-system            kube-scheduler-minikube                     1/1     Running     1          89d
kube-system            registry-65xp4                              1/1     Running     1          89d
kube-system            registry-proxy-dqn2b                        1/1     Running     1          89d
kube-system            storage-provisioner                         1/1     Running     5          89d
kubernetes-dashboard   dashboard-metrics-scraper-dc6947fbf-bp98k   1/1     Running     0          13d
kubernetes-dashboard   kubernetes-dashboard-58b79879c5-2w2wl       1/1     Running     0          13d


#+begin_src shell :results raw
  kubectl label node minikube disk=hdd --overwrite
#+end_src

#+RESULTS:
node/minikube labeled
node/minikube not labeled


#+begin_src shell :results raw
kubectl get pods
#+end_src

#+RESULTS:
NAME                READY   STATUS        RESTARTS   AGE
ssd-monitor-vgqwv   1/1     Terminating   0          10m
NAME                READY   STATUS    RESTARTS   AGE
ssd-monitor-vgqwv   1/1     Running   0          9m4s
NAME                READY   STATUS    RESTARTS   AGE
ssd-monitor-vgqwv   1/1     Running   0          8m30s

#+begin_src shell :results raw
kubectl get ds 
#+end_src

#+RESULTS:
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   0         0         0       0            0           disk=ssd        16m

#+begin_src shell :results raw
kubectl delete ds ssd-monitor 
#+end_src

#+RESULTS:
daemonset.apps "ssd-monitor" deleted

#+begin_src shell :results raw
kubectl get ds 
#+end_src

#+RESULTS:

* Job
- Used to manage pods that run a task and exits on completion
- Exmple would be ETL job, lots of prow jobs

#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/job-exporter.yaml
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: batch-job
  spec:
    template:
      metadata:
        labels:
          app: batch
      spec:
        restartPolicy: OnFailure
        containers:
        - name: main
          image: luksa/batch-job
#+end_src

#+begin_src shell :results raw
kubectl apply -f yaml/job-exporter.yaml
#+end_src

#+RESULTS:
job.batch/batch-job created

#+begin_src shell :results raw
kubectl get po
#+end_src

#+RESULTS:
NAME              READY   STATUS      RESTARTS   AGE
batch-job-rmf74   0/1     Completed   0          2m9s
NAME              READY   STATUS    RESTARTS   AGE
batch-job-rmf74   1/1     Running   0          2m2s
NAME              READY   STATUS    RESTARTS   AGE
batch-job-rmf74   1/1     Running   0          66s
NAME              READY   STATUS              RESTARTS   AGE
batch-job-rmf74   0/1     ContainerCreating   0          5s

#+begin_src shell :results raw
kubectl logs batch-job-rmf74 
#+end_src

#+RESULTS:
Tue Nov 17 10:45:05 UTC 2020 Batch job starting
Tue Nov 17 10:47:05 UTC 2020 Finished succesfully


#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/multi-completion-parralel-batch-job.yaml
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: multi-completion-batch-job
  spec:
    completions: 5
    parallelism: 2
    template:
      metadata:
        labels:
          app: batch
      spec:
        restartPolicy: OnFailure
        containers:
        - name: main
          image: luksa/batch-job
#+end_src

#+begin_src shell :results raw
kubectl apply -f yaml/multi-completion-parralel-batch-job.yaml
#+end_src

#+RESULTS:
job.batch/multi-completion-batch-job created

#+begin_src shell :results raw
kubectl get po
#+end_src

#+RESULTS:
NAME                               READY   STATUS              RESTARTS   AGE
batch-job-rmf74                    0/1     Completed           0          22h
multi-completion-batch-job-9x6qn   0/1     ContainerCreating   0          4s
multi-completion-batch-job-l9pn5   0/1     ContainerCreating   0          4s
NAME              READY   STATUS      RESTARTS   AGE
batch-job-rmf74   0/1     Completed   0          22h

Not sure why this scaling is not working, it says it is not recognizing the resourcem perhaps scale got depricated?
TODO: look into manually scaling, more for interest than real use
#+begin_src shell :results raw
  kubectl scale job multi-completion-batch-job --replicas 3
#+end_src

#+RESULTS:
NAME                         COMPLETIONS   DURATION   AGE
batch-job                    1/1           2m6s       22h
multi-completion-batch-job   2/5           3m9s       3m9s

** cronjob
Works roughly same as linux cronjob, but runs k8s job

Lets write one, see what it does
#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/cronjob.yaml
  apiVersion: batch/v1beta1
  kind: CronJob
  metadata:
    name: batch-job-every-fifteen-minutes
  spec:
    schedule: "0,15,30,45 * * * *"
    jobTemplate:
      spec:
        template:
          metadata:
            labels:
              app: periodic-batch-job
          spec:
            restartPolicy: OnFailure
            containers:
            - name: main
              image: luksa/batch-job
#+end_src

#+begin_src shell :results raw
  kubectl delete -f yaml/cronjob.yaml
#+end_src

#+RESULTS:
cronjob.batch "batch-job-every-fifteen-minutes" deleted
cronjob.batch/batch-job-every-fifteen-minutes created

#+begin_src shell :results raw
  kubectl get cronjobs
#+end_src

#+RESULTS:
NAME                              SCHEDULE             SUSPEND   ACTIVE   LAST SCHEDULE   AGE
batch-job-every-fifteen-minutes   0,15,30,45 * * * *   False     0        <none>          38s
NAME                         COMPLETIONS   DURATION   AGE
batch-job                    1/1           2m6s       23h
multi-completion-batch-job   5/5           6m20s      26m

* Services
Services allows k8s to assign a singel ip/port to multiple pods. It can be external or internal, but the point is to give requests a single place to look for pods etc fronted by a service
We used kubectl expose earlier to give us access to the kubia pods, expose is a simple way to use kubectl to create a service

Now lets build a service manually


#+begin_src yaml :tangle yaml/kubia-svc.yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: kubia
  spec:
    ports:
    - port: 80
      targetPort: 8080
    selector:
      app: kubia
#+end_src

#+begin_src shell :results raw
  kubectl create -f yaml/kubia-svc.yaml
#+end_src

#+RESULTS:
service/kubia created


#+begin_src shell :results raw
  kubectl get svc
#+end_src

#+RESULTS:
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP   11d
kubia        ClusterIP   10.103.5.57   <none>        80/TCP    39s

You can see it only has an internal cluster-ip, that is because the primary purpose of service is to expose groups of pods to other pods
ok I no longer have any of the pods running, lets re-apply our replication controller we deleted
#+begin_src shell :results raw
kubectl create -f yaml/kubia-rc.yaml 
#+end_src

#+RESULTS:
replicationcontroller/kubia created

#+begin_src shell :results raw
kubectl get pods
#+end_src

#+RESULTS:
NAME                               READY   STATUS      RESTARTS   AGE
batch-job-rmf74                    0/1     Completed   0          6d12h
kubia-76n42                        1/1     Running     0          59s
kubia-8889t                        1/1     Running     0          59s
kubia-rq4j6                        1/1     Running     0          59s
multi-completion-batch-job-765n2   0/1     Completed   0          5d13h
multi-completion-batch-job-9x6qn   0/1     Completed   0          5d13h
multi-completion-batch-job-l9pn5   0/1     Completed   0          5d13h
multi-completion-batch-job-rkdc2   0/1     Completed   0          5d13h
multi-completion-batch-job-tvwp9   0/1     Completed   0          5d13h

OK we have 3 running pods. Now how do we see if they are working if there is no external ip, 3 ways
- Create a pod that can query the service and log results
- ssh into the node and then curl
- kubectl exec into a pod and try from there
I am going to try the exec route

#+begin_src shell :results raw
kubectl exec kubia-76n42 -- curl -s http://10.103.5.57
#+end_src

#+RESULTS:
You've hit kubia-rq4j6
You've hit kubia-8889t
You've hit kubia-rq4j6
You've hit kubia-rq4j6
You've hit kubia-8889t

NICE, love exec such a simple way to run things on a pod
I ran the command a few times, As we can see it hits the pods randomly as the service round robins 

Why the double dash?
The double dash (--) in the command signals the end of command options for kubectl everything after the -- is what we are executing in the pod

you do not have to use the -- but any - that shows up will be treated as a kubectl flag

Ok, but can I add afinity so I will always hit the same pod? yes, yes I can simply add
#+begin_example
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  sessionAffinity: ClientIP
#+end_example
This makes the service proxy redirect all requests originating from the same client IP to the same pod
There is only 2 types of affinity, clientIP and none, there is no cookie based affinity because service is tcp and UDP but cookies is a http construct

Now lets go create a service with multiple ports

#+begin_src yaml :tangle yaml/kubia-svc-multi-port.yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: kubia-multi
  spec:
    ports:
    - name: http
      port: 80
      targetPort: 8080
    - name: https
      port: 900
      targetPort: 8443
    selector:
      app: kubia
#+end_src

Lets go add multiple ports to our pods

#+begin_src yaml :tangle yaml/kubia-rc-multi.yaml
  apiVersion: v1
  kind: ReplicationController
  metadata:
    name: kubia
  spec:
    replicas: 3
    selector:
      app: kubia
    template:
      metadata:
        labels:
          app: kubia
      spec:
        containers: 
        - name: kubia
          image: bernokl/kubia-multi-port
          ports:
            - containerPort: 8080
              name: http
            - containerPort: 8443
              name: https
#+end_src

OK I first need to go delete the service we have
#+begin_src shell :results raw
kubectl apply -f yaml/kubia-svc-multi-port.yaml 
kubectl apply -f yaml/kubia-rc-multi.yaml 
#+end_src

#+RESULTS:
service/kubia-multi created
replicationcontroller/kubia created
service "kubia-multi" deleted
replicationcontroller "kubia" deleted
replicationcontroller/kubia created
service/kubia-multi created


#+begin_src shell :results raw
kubectl get svc
#+end_src

#+RESULTS:
NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
kubernetes    ClusterIP   10.96.0.1        <none>        443/TCP          12d
kubia-multi   ClusterIP   10.103.117.154   <none>        80/TCP,900/TCP   5s
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   12d
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes    ClusterIP   10.96.0.1      <none>        443/TCP          12d
kubia-multi   ClusterIP   10.107.93.75   <none>        80/TCP,900/TCP   4s

#+begin_src shell :results raw
kubectl get pods
#+end_src

#+RESULTS:
NAME                               READY   STATUS      RESTARTS   AGE
batch-job-rmf74                    0/1     Completed   0          6d16h
kubia-42tp2                        1/1     Running     0          18s
kubia-h8kds                        1/1     Running     0          18s
kubia-rcvn9                        1/1     Running     0          18s
NAME                               READY   STATUS        RESTARTS   AGE
batch-job-rmf74                    0/1     Completed     0          6d16h
kubia-4hhrj                        0/1     Terminating   0          62m
NAME                               READY   STATUS              RESTARTS   AGE
batch-job-rmf74                    0/1     Completed           0          6d15h
kubia-4hhrj                        0/1     ContainerCreating   0          5s
kubia-d6d6q                        1/1     Running             0          5s
kubia-rfzl9                        0/1     ContainerCreating   0          5s

#+begin_src shell :results raw
# this one times out 
    kubectl exec kubia-42tp2 -- curl -k  http://10.103.117.154:900
# this one works
    kubectl exec kubia-42tp2 -- curl -k  http://10.103.117.154
#+end_src

#+RESULTS:
You've hit kubia-rcvn9

Aite after about 30 minutes of messing with curl it occured to me to go back to the node app that is running in the pod, and the silly bugger is only listening on port 8080
OK I updated the file, did docker build/push, updated the pod definition above to pull the new image 
Changes to the app.js file:
var www = http.createServer(handler);
www.listen(8080);
www.listen(8443);

But going into the pod I do not see it listening to 8443 I am not spending the time to figure it out. to make the change I have to update and push the docker image again, I dont wanna
On to the next steps 

One more really handy thing, I can replace the targetPort in the service spec with the name of the port in my pod definition, that way if the containerPort changes I do not have to change it in the service
For example the targetPorts below were replaced with http and https, the names I gave those ports in pod spec, cool eh?
#+begin_example
apiVersion: v1
kind: Service
spec:
  ports:
  - name: http
port: 80
    targetPort: http
  - name: https
port: 443
    targetPort: https
#+end_example


Ok it sounds like a pod can find its service, becuase there are env vars set when a pod gets created,
If I created the pods and then the service I would need to delete them so they can inherit the service env vars, but these pods have been killed a few times soooo
I am going to exec into the pods and see what env is set

#+begin_src shell :results raw 
kubectl exec kubia-42tp2 env
#+end_src

#+RESULTS:
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubia-42tp2
KUBIA_MULTI_SERVICE_HOST=10.103.117.154
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBIA_MULTI_PORT=tcp://10.103.117.154:80
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBIA_MULTI_PORT_900_TCP_PORT=900
KUBERNETES_SERVICE_PORT=443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PORT=443
KUBIA_MULTI_PORT_80_TCP_PROTO=tcp
KUBIA_MULTI_PORT_80_TCP_ADDR=10.103.117.154
KUBIA_MULTI_SERVICE_PORT_HTTPS=900
KUBIA_MULTI_PORT_80_TCP=tcp://10.103.117.154:80
KUBIA_MULTI_PORT_80_TCP_PORT=80
KUBIA_MULTI_PORT_900_TCP=tcp://10.103.117.154:900
KUBIA_MULTI_PORT_900_TCP_PROTO=tcp
KUBIA_MULTI_PORT_900_TCP_ADDR=10.103.117.154
KUBIA_MULTI_SERVICE_PORT=80
KUBIA_MULTI_SERVICE_PORT_HTTP=80
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
NPM_CONFIG_LOGLEVEL=info
NODE_VERSION=7.10.1
YARN_VERSION=0.24.4
HOME=/root

O fancy I see KUBIA_MULTI_SERVICE_HOST=10.103.117.154 and lots of other vars with ports etc
that means the pod came up with an env var pointing to its service

K8s come with built in dns servers if we had a backend-database pod, any pod on the cluster can find it with, lets look at a backend-database service:
backend-database.default.svc.cluster.local

backend-database = service name
default = namespace
svc.cluster.local = configurable cluster domain suffics

if the service is in the same namespace as the pod that needs ityou only need backend-database, how badass is that any service you start is automatically added to internal dns

Lets go look at it for a bit, lets exec into a container and see what we find curlining things by their dns names

O bother this is interactive so cant do here, going to run this from cli, might even follow up with a couple of curls like we did above
#+begin_src shell :results raw
kubectl exec -it kubia-42tp2 bash 
#+end_src

#+RESULTS:

OK here is some output from a few tests I did in shell
#+begin_example
$ kubectl exec -it kubia-42tp2 -- bash
root@kubia-42tp2:/# curl http://kubia-multi.default.svc.cluster.local
You've hit kubia-h8kds
root@kubia-42tp2:/# curl http://kubia-multi.default
You've hit kubia-h8kds
root@kubia-42tp2:/# curl http://kubia-multi
You've hit kubia-rcvn9
root@kubia-42tp2:/# cat /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
#+end_example

Note each pod gets /etc/resolv.conf updated so it first tries the clusterDNS

Just to be silly lets run the above commands using the same exec we did earlier, gonna pile them all in the same codeblock


#+begin_src shell :results raw
    kubectl exec kubia-42tp2 -- curl -s http://kubia-multi.default.svc.cluster.local
    kubectl exec kubia-42tp2 -- curl -s http://kubia-multi.default
    kubectl exec kubia-42tp2 -- curl -s http://kubia-multi
    kubectl exec kubia-42tp2 -- cat /etc/resolv.conf
#+end_src

#+RESULTS:
You've hit kubia-h8kds
You've hit kubia-rcvn9
You've hit kubia-h8kds
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5

NICE, did I mention I love doing this in org?

ALSO important thing to remember, you can not ping service ips, that is because service ips are virtual ips and only has meaning when it is combined with the service port

Services can also be pointed to services running outside of the cluster. 
If you connect a service to an external service you get to take advantage of load balancing and service discovery
This means that pods in the cluster can now take advantage of external services the same way they would for internal service.


** service endpoints
These sit between services and the resource it is pointing to, lets go describe our service to see if we can see endpoints

#+begin_src shell :results raw
kubectl describe svc kubia-multi
#+end_src

#+RESULTS:
Name:              kubia-multi
Namespace:         default
Labels:            <none>
Annotations:       Selector:  app=kubia
Type:              ClusterIP
IP:                10.103.117.154
Port:              http  80/TCP
TargetPort:        8080/TCP
Endpoints:         172.17.0.10:8080,172.17.0.8:8080,172.17.0.9:8080
Port:              https  900/TCP
TargetPort:        8443/TCP
Endpoints:         172.17.0.10:8443,172.17.0.8:8443,172.17.0.9:8443
Session Affinity:  None
Events:            <none>


The selector above is used to populate the endpoints, it is a list of IP's and ports, endpoints is like any other k8s resource, lets go use kubectl to get the endpoints
#+begin_src shell :results raw
kubectl get endpoints kubia-multi 
#+end_src

#+RESULTS:
NAME          ENDPOINTS                                                      AGE
kubia-multi   172.17.0.10:8443,172.17.0.8:8443,172.17.0.9:8443 + 3 more...   5h55m

Curious what it looks like if we try to describe those? me to :)
#+begin_src shell :results raw
kubectl describe endpoints kubia-multi 
#+end_src

#+RESULTS:
Name:         kubia-multi
Namespace:    default
Labels:       <none>
Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2020-11-24T02:55:59Z
Subsets:
  Addresses:          172.17.0.10,172.17.0.8,172.17.0.9
  NotReadyAddresses:  <none>
  Ports:
    Name   Port  Protocol
    ----   ----  --------
    https  8443  TCP
    http   8080  TCP

Events:  <none>

NICE, damn I love this platform, that is so clean. 

You can create a service without a selector, but that means the endpoints will not be populated
Lets go set one up that has no selector to see how we add endpoints

#+begin_src yaml :tangle yaml/external-service.yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: external-service
  spec:
    ports:
    - port: 80
#+end_src

Now because there is no selector no endpoints will be created, but we want to point it to a external service so that is good, lets go define our own endpoints
#+begin_src yaml :tangle yaml/external-service-endpoints.yaml
  apiVersion: v1
  kind: Endpoints
  metadata:
    name: external-service
  subsets:
    - addresses:
      - ip: 11.11.11.11
      - ip: 22.22.22.22
      ports:
      - port: 80
 #+end_src
 VERY IMPORTANT: notice that the enpoint has the same name as the service

once the above 2 have been applied we can poll the external service from any pod. If a pod comes up after the service was created it will have env for that service or else we can use dns
This is amazing, if you ever move the external services into the cluster simply add a selector

Another badass bit is that you can simply refer to the service by its external FQDN
here is a quick example
#+begin_src yaml :tangle yaml/external-service-externalname.yaml
  apiVersion: v1
  kind: Service
  metadata: 
    name: external-service
  spec:
    type: ExternalName
    externalName: someapi.somecompany.com
    ports:
    - port: 80
#+end_src
One thing to note this kind of service will bypass the service proxy layer, pods will simply get directly forwarded to the external FQDN mening it will not even get a clusterIP

** exposing your service to the world
3 ways:
- NodePort: each node opens a port that will redirect all traffic to pods that are part of the service
- LoadBalancer: same as NodePort, but using a cloud provider lb to point to traffic to the nodePort accross all nodes
- Ingress: This will allow you to expose multiple services through a single ip, much more like a NAT 

KEEP IN MIND ALL 3 IS A TYPE OF SERVICE!

*** NodePort 
lets go manually define a nodeport:
#+begin_src yaml :tangle yaml/kubia-svc-nodeport.yaml
apiVersion: v1 
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30123
  selector:
    app: kubia
#+end_src

Ok lets go apply
#+begin_src shell :results raw
kubectl apply -f yaml/kubia-svc-nodeport.yaml 
#+end_src

#+RESULTS:
service/kubia-nodeport created

#+begin_src shell :results raw 
kubectl get svc kubia-nodeport
#kubectl get po --selector=app=kubia
#+end_src

#+RESULTS:
NAME             TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
kubia-nodeport   NodePort   10.98.80.58   <none>        80:30123/TCP   5m3s
NAME          READY   STATUS    RESTARTS   AGE
kubia-42tp2   1/1     Running   0          7h2m
kubia-h8kds   1/1     Running   0          7h2m
kubia-rcvn9   1/1     Running   0          7h2m

#+begin_src shell :results raw
minikube ip
#+end_src

#+RESULTS:
192.168.64.2


#+begin_src shell :results raw
curl http://192.168.64.2:30123
#+end_src

#+RESULTS:
You've hit kubia-42tp2

one thing to think about when you do this in the cloud you need to remember to add firewall rules, thought this was interesting way to do it in gcloud
#+begin_example
gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123
#+end_example

Something really handy, files can ge output to all sorts of formats, one to remember is jasonpath which allows you to specify the element you want to see
This example will get the ips of all our nodes

#+begin_src shell :results raw
  #kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
  kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
#+end_src

#+RESULTS:

mkay, not sure why it is not working, but you can read more about jsonpath on http://kubernetes.io/docs/user-guide/jsonpath 
basically allows you to traverse json like xpath lets you traverse xml

I suspect it is because I am using minikube.. ONWARDS!

*** LoadBalancer
k8s will provision a cloud provided loadbalancer automatically, because I am working on minikube, I am not going to try this, but will revisit it at some point

#+begin_src yaml :tangle yaml/kubia-svc-loadbalancer.yaml
apiVersion: v1 
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
#+end_src
now I am curious, lets tangle this sucker, apply it and see what happens

#+begin_src shell :results raw
kubectl apply -f yaml/kubia-svc-loadbalancer.yaml 
#+end_src

#+RESULTS:
service/kubia-loadbalancer created

Wut!?!?
ok lets go see what that means

#+begin_src shell :results raw
kubectl get svc kubia-loadbalancer
#+end_src

#+RESULTS:
NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubia-loadbalancer   LoadBalancer   10.100.52.153   <pending>     80:30435/TCP   67s

aite it is pending a external ip, lets give it 5 minutes and then come back to this see if it managed to create something, can not think how, but this is fun.

One thing to note is that LB is just an extention of nodeport (NP) if you set up lb, you will still end up with open ports on nodes that you can access as long as firewall rules are set
LB is better than NP, because browsing directly to nodes means the node has to be up, but with LB if one node fails it will just point at a different one

One thing to note the nodeport service runs on all nodes, but it will redirect traffic to any other node so it could mean an extra network hop, force nodeport to always forward
traffic to itself first to avoid that extra hop, to do this you just add the following to the spec
#+begin_example
spec:
  externalTrafficPolicy: Local
#+end_example
This also helps with preservation of the client ip, if you allow the extra hop the ip the call is coming from is the nodedport node it hit, but if it is local it will preserve the clientip

*** Ingress
Ingress is a singe entry point into the cluster
Ingress will allow multiple services on a single IP
Ingress operates at the application level (HTTP) and can provide things like cookie-based affinity which services can not do

Ingress is not a default feature, but we can enable it, lets go see
#+begin_src shell :results raw
minikube addons list 
#+end_src

#+RESULTS:
|-----------------------------+----------+------------|
| ADDON NAME                  | PROFILE  | STATUS     |
|-----------------------------+----------+------------|
| ambassador                  | minikube | disabled   |
| csi-hostpath-driver         | minikube | disabled   |
| dashboard                   | minikube | enabled ✅ |
| default-storageclass        | minikube | enabled ✅ |
| efk                         | minikube | disabled   |
| freshpod                    | minikube | disabled   |
| gcp-auth                    | minikube | disabled   |
| gvisor                      | minikube | disabled   |
| helm-tiller                 | minikube | disabled   |
| ingress                     | minikube | enabled ✅ |
| ingress-dns                 | minikube | disabled   |
| istio                       | minikube | disabled   |
| istio-provisioner           | minikube | disabled   |
| kubevirt                    | minikube | disabled   |
| logviewer                   | minikube | disabled   |
| metallb                     | minikube | disabled   |
| metrics-server              | minikube | disabled   |
| nvidia-driver-installer     | minikube | disabled   |
| nvidia-gpu-device-plugin    | minikube | disabled   |
| olm                         | minikube | disabled   |
| pod-security-policy         | minikube | disabled   |
| registry                    | minikube | enabled ✅ |
| registry-aliases            | minikube | disabled   |
| registry-creds              | minikube | disabled   |
| storage-provisioner         | minikube | enabled ✅ |
| storage-provisioner-gluster | minikube | disabled   |
| volumesnapshots             | minikube | disabled   |
|-----------------------------+----------+------------|

OO it looks like the ingress is already enabled, must have changed in the last few versions, just as a note to enable a addon use the following format
#+begin_example
minikube addons enable ingress
#+end_example

Lets go see if we can find the ingress pod
#+begin_src shell :results raw 
kubectl get po -A
#+end_src

#+RESULTS:
NAMESPACE              NAME                                        READY   STATUS      RESTARTS   AGE
default                kubia-42tp2                                 1/1     Running     0          17h
default                kubia-h8kds                                 1/1     Running     0          17h
default                kubia-rcvn9                                 1/1     Running     0          17h
kube-system            coredns-66bff467f8-96gv4                    1/1     Running     2          96d
kube-system            etcd-minikube                               1/1     Running     1          20d
kube-system            ingress-nginx-admission-create-fpcvv        0/1     Completed   0          96d
kube-system            ingress-nginx-admission-patch-6qfcz         0/1     Completed   0          96d
kube-system            ingress-nginx-controller-799dcd5d57-cmd68   1/1     Running     1          20d
kube-system            kube-apiserver-minikube                     1/1     Running     2          96d
kube-system            kube-controller-manager-minikube            1/1     Running     2          96d
kube-system            kube-proxy-d892r                            1/1     Running     2          96d
kube-system            kube-scheduler-minikube                     1/1     Running     2          96dG
kube-system            registry-65xp4                              1/1     Running     2          96d
kube-system            registry-proxy-dqn2b                        1/1     Running     2          96d
kube-system            storage-provisioner                         1/1     Running     8          96d
kubernetes-dashboard   dashboard-metrics-scraper-dc6947fbf-bp98k   1/1     Running     1          20d
kubernetes-dashboard   kubernetes-dashboard-58b79879c5-2w2wl       1/1     Running     2          20d


There it is I see the ingress-nginx-controller running
It looks like it is based on nginx, lets go see if we can add a ingress resource to our cluster

Note the type is ingress not service, even though it is a service
#+begin_src yaml :tangle yaml/kubia-ingress.yaml
    apiVersion: networking.k8s.io/v1 
    kind: Ingress
    metadata:
      name: kubiaingress
    spec:
      rules:
      - host: kubia.example.com
        http:
          paths:
          - path: /
            backend:
              service:
               name: kubia-nodeport
               port: 80
#+end_src

This defines an Ingress with a single rule, which makes sure all HTTP requests received by the Ingress controller, in which the host kubia.example.com is requested, will be sent to the kubia-nodeport service on port 80.
Note that even though we are tying it to a service, this is not required by k8s 

ok lets apply this bad boy
#+begin_src shell :results raw
kubectl apply -f yaml/kubia-ingress.yaml 
#+end_src

#+RESULTS:
ingress.extensions/kubiaingress created

#+begin_src shell :results raw
kubectl get ingress
#+end_src

#+RESULTS:
NAME           CLASS    HOSTS               ADDRESS        PORTS   AGE
kubiaingress   <none>   kubia.example.com   192.168.64.2   80      13m

Do you know what you just did? you made an ingress, if you add /etc/hosts entry to have that address point to that host you can now get to pods through http:kubia.example.com  ..... can I have a woop woop?

For gigles lets do it with curl
#+begin_src shell :results raw
curl http://kubia.example.com 
#+end_src

#+RESULTS:
You've hit kubia-rcvn9

yeah I did! HA!

And here you see how to add mutltiple services to the ingress
#+begin_example
- host: kubia.example.com
http: paths:
      - path: /kubia
        backend:
          serviceName: kubia
          servicePort: 80
      - path: /foo
        backend:
          serviceName: bar
          servicePort: 80
#+end_example
In this example both still come in on kubia.example, but will be redirected based on the path in the url
Lets see wht it looks like if there are multiple HTTP requests
#+begin_example
spec: rules:
  - host: foo.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: foo
          servicePort: 80
  - host: bar.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: bar
          servicePort: 80
#+end_example

**** Configuring the ingress to hanle tls
pods typically do not get tls termination the ingress handles all TLS, lets give it a go
#+begin_src shell :results raw
  openssl genrsa -out tls.key 2048 
openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj /CN=kubia.example.com
#+end_src

#+RESULTS:

ok that did not work here, so I ran it in cli and I see the two files, lets go create secrets with it.
#+begin_src shell :results raw
kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key 
#+end_src

#+RESULTS:
secret/tls-secret created


###+begin_src yaml :tangle yaml/kubia-ingress-tls.yaml
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: kubiaingress
  spec:
    tls:
    - hosts:
      - kubia.example.com
      secretName: tls-secret
    rules:
      - host: kubia.example.com
        http:
          paths:
          - path: /
            backend:
              serviceName: kubia-nodeport:
              servicePort: 80
###+end_src

#+begin_src yaml :tangle yaml/kubia-ingress-tls.yaml
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: kubiaingress
  spec:
    tls:
      - hosts:
        - kubia.example.com
        secretName: tls-secret
    rules:
      - host: kubia.example.com
        http:
          paths:
          - path: /
            backend:
              serviceName: kubia-nodeport
              servicePort: 80
#+end_src

#+begin_src shell :results raw
kubectl apply -f yaml/kubia-ingress-tls.yaml 
#+end_src

#+RESULTS:
ingress.networking.k8s.io/kubiaingress configured
ingress.networking.k8s.io "kubia" deleted
ingress.networking.k8s.io/kubia created

#+begin_src shell :results raw
kubectl get ingress
#+end_src

#+RESULTS:
NAME           CLASS    HOSTS               ADDRESS        PORTS     AGE
kubiaingress   <none>   kubia.example.com   192.168.64.2   80, 443   4h41m

Nice, I see 443 Lets test
#+begin_src shell :results raw
curl -k -v https://kubia.example.com/kubia 
#+end_src

#+RESULTS:
#+begin_example 
*   Trying 192.168.64.2...
* TCP_NODELAY set
* Connected to kubia.example.com (192.168.64.2) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/cert.pem
  CApath: none
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=kubia.example.com
*  start date: Nov 24 22:41:05 2020 GMT
*  expire date: Nov 19 22:41:05 2021 GMT
*  issuer: CN=kubia.example.com
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x7fe5c800b600)
> GET /kuia HTTP/2
> Host: kubia.example.com
> User-Agent: curl/7.54.0
> Accept: */*
>
* Connection state changed (MAX_CONCURRENT_STREAMS updated)!
< HTTP/2 200
< server: nginx/1.19.1
< date: Wed, 25 Nov 2020 02:06:09 GMT
<
You've hit kubia-42tp2
* Connection #0 to host kubia.example.com left intact
#+end_example

Got to 5.5 Signaling when a pod is ready to accept connections

**** Readiness probes
Same as liveness probes k8s allows you to define a readiness probe, it determines whether a pod is ready for client requests
Types of readiness probes
- exec, a process is executed, the status is determined by that processes exit status code
- HTTP GET probe sends HTTP get request tot eh container, the HTTP status codes determines the status
- A TCP socket probe which opens a TCP connection to a specified port, if the connection is established the container is considered ready

An important distinction between readiness probes and liveness probes, when a readiness probe fail the container does not get killed and repled like it does with liveness
The pod is just removed from the endpoints object so it will not receive traffic so it does not get traffic until it recovers

Remember you can edit live configs with
#+begin_example
kubectl edit rc kubia
#+end_example
I am not doing it here becuase it opens a new interactive window to make the update one
Here is what I am adding
#+begin_example
apiVersion: v1
kind: ReplicationController
...
spec:
  ...
  template:
... spec:
      containers:
      - name: kubia
        image: luksa/kubia
        readinessProbe:
          exec:
            command:
            - ls
            - /var/ready
#+end_example

This gives us a readiness probe we can easilly manipulate, if the file exists the pod will run if it is deleted it will fail
One thing to note that updating the ReplicationController pod definition is not causing existing pods to fail, they will only fail once they get re-created

#+begin_src shell :results raw
kubectl get po 
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE
kubia-42tp2   1/1     Running   0          25h
kubia-h8kds   1/1     Running   0          25h
kubia-rcvn9   1/1     Running   0          25h

Lets delete one and see if it changes when it comes back

#+begin_src shell :results raw
kubectl delete po kubia-42tp2
#+end_src

#+RESULTS:
pod "kubia-42tp2" deleted

#+begin_src shell :results raw
kubectl get po 
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE
kubia-h8kds   1/1     Running   0          25h
kubia-nj8kd   0/1     Running   0          6m5s
kubia-rcvn9   1/1     Running   0          25h

O snap it is no longer ready, lets go touch the file

#+begin_src shell :results raw
kubectl exec kubia-nj8kd -- touch /var/ready 
#+end_src

#+RESULTS:

#+begin_src shell :results raw
kubectl get po 
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE
kubia-h8kds   1/1     Running   0          25h
kubia-nj8kd   1/1     Running   0          7m33s
kubia-rcvn9   1/1     Running   0          25h

Ahhh it is back the probe works! how cool is that?

lets see if I cause the other two to fail, will I ever hit them?
Gonna try a few commands in a row, lets see how we go

#+begin_src shell :results raw
kubectl delete po kubia-h8kds
kubectl delete po kubia-rcvn9
kubectl get po 
#+end_src

#+RESULTS:
pod "kubia-h8kds" deleted
pod "kubia-rcvn9" deleted
NAME          READY   STATUS    RESTARTS   AGE
kubia-dh8kt   0/1     Running   0          74s
kubia-nj8kd   1/1     Running   0          12m
kubia-zbk5r   0/1     Running   0          32s

#+begin_src shell :results raw
curl http://192.168.64.2:30123
curl http://192.168.64.2:30123
curl http://192.168.64.2:30123
curl http://192.168.64.2:30123
curl http://192.168.64.2:30123
#+end_src

#+RESULTS:
You've hit kubia-nj8kd
You've hit kubia-nj8kd
You've hit kubia-nj8kd
You've hit kubia-nj8kd
You've hit kubia-nj8kd

As expected we only hit the healthy pods, that is good to confirm 
#+begin_src shell :results raw
kubectl exec kubia-dh8kt -- touch /var/ready 
kubectl exec kubia-zbk5r -- touch /var/ready 
#+end_src

#+RESULTS:

#+begin_src shell :results raw
kubectl get po 
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE
kubia-dh8kt   1/1     Running   0          13m
kubia-nj8kd   1/1     Running   0          25m
kubia-zbk5r   1/1     Running   0          12m

#+begin_src shell :results raw
curl http://192.168.64.2:30123
curl http://192.168.64.2:30123
curl http://192.168.64.2:30123
curl http://192.168.64.2:30123
curl http://192.168.64.2:30123
#+end_src

#+RESULTS:
You've hit kubia-nj8kd
You've hit kubia-dh8kt
You've hit kubia-dh8kt
You've hit kubia-dh8kt
You've hit kubia-zbk5r

Aaannndd hit them all, sweet

An easy way to remove a single pod is to just label it as enabled
Add the label to my rc label selector
#+begin_src shell :results value verbatim
kubectl label po kubia-zbk5r enabled=true --overwrite 
#+end_src

#+RESULTS:
: pod/kubia-zbk5r labeled

#+begin_src shell :results value verbatim
kubectl get po --show-labels -n default 
#+end_src

#+RESULTS:
: NAME          READY   STATUS    RESTARTS   AGE   LABELS
: kubia-dh8kt   1/1     Running   0          27m   app=kubia,enabled=false
: kubia-nj8kd   1/1     Running   0          38m   app=kubia,enabled=true
: kubia-zbk5r   1/1     Running   0          26m   app=kubia,enabled=true

#+begin_src shell :results raw
kubectl get rc 
#+end_src

#+RESULTS:
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       25h

#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/kubia-rc.yaml
  apiVersion: v1
  kind: ReplicationController
  metadata:
    name: kubia
  spec:
    replicas: 3
    selector:
      app: kubia
      enabled: true
    template:
      metadata:
        labels:
          app: kubia
      spec:
        containers: 
        - name: kubia
          image: bernokl/kubia
          ports:
            - containerPort: 8080
    #+end_src

#+begin_src shell :results raw
kubectl apply -f yaml/kubia-rc.yaml 
#+end_src

#+RESULTS:

#+begin_src shell :results raw
kubectl get po
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE
kubia-dh8kt   1/1     Running   0          32m
kubia-nj8kd   1/1     Running   0          44m
kubia-zbk5r   1/1     Running   0          32m

mmmm I would have thought it would fail, i am not sure, not really in the mood to troubleshoot it, to me that looks like it should have failed

So a headless service is if you create a service without a clusterip, it will still be the parent, but instead of single ip for all pods it will create A records with ip's for each backing pod
This means the client can connect to any of the pods associated with the service based on the individual IPs

Lets go define one just to see what it would look like


#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/kubia-svc-headless.yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: kubia-headless
  spec:
    clusterIP: None
    ports:
    - port: 80
      targetPort: 8080
    selector:
      app: kubia
#+end_src

Ok lets go apply it see what we see

#+begin_src shell :results raw
kubectl apply -f yaml/kubia-svc-headless.yaml
#+end_src

#+RESULTS:
service/kubia-headless created

#+begin_src shell :results raw
kubectl describe svc kubia-headless 
#+end_src

#+RESULTS:
Name:              kubia-headless
Namespace:         default
Labels:            <none>
Annotations:       Selector:  app=kubia
Type:              ClusterIP
IP:                None
Port:              <unset>  80/TCP
TargetPort:        8080/TCP
Endpoints:         172.17.0.10:8080,172.17.0.11:8080,172.17.0.9:8080
Session Affinity:  None
Events:            <none>


-----THIS IS AWESOME----
Directly run a container
OOOO thisis badass, this will let you run a image in your cluster (in this case I needed something with dnsutils rather than install it on one of the exsisting pods
I simply run the below to create the single pod that is not backed by RC or anything else
#+begin_src shell :results raw
kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1 --command -- sleep infinity
#+end_src

#+RESULTS:
pod/dnsutils created

#+begin_src shell :results raw
kubectl get po
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE
dnsutils      1/1     Running   0          43s
kubia-dh8kt   1/1     Running   0          4d17h
kubia-nj8kd   1/1     Running   0          4d17h
kubia-zbk5r   1/1     Running   0          4d17h

k lets hop into our container so we can go do some dns lookups on our new headless service
#+begin_src shell :results raw
kubectl exec dnsutils nslookup kubia-headless 
#+end_src

#+RESULTS:
Server:		10.96.0.10
Address:	10.96.0.10#53

Name:	kubia-headless.default.svc.cluster.local
Address: 172.17.0.11
Name:	kubia-headless.default.svc.cluster.local
Address: 172.17.0.10
Name:	kubia-headless.default.svc.cluster.local
Address: 172.17.0.9


How about them applis instead of a single ip for the service we get the individual pod ips.
Ok lets see what happens if I look up the same thing for one with a clusterIP

#+begin_src shell :results raw
kubectl exec dnsutils nslookup kubia-multi
#+end_src

#+RESULTS:
Server:		10.96.0.10
Address:	10.96.0.10#53

Name:	kubia-multi.default.svc.cluster.local
Address: 10.103.117.154

As expected, nice

In practice a headless service will work exactly the same as a regular service, 
you can still hit the pods by looking up the service, 
but instead of the loadbalancing happening at the service level it round robins using DNS round robin

SERVICE TROUBLESHOOTING TIPS
1) Make sure you can connect to the service clusterIP from inside the cluster not outside
2) Dont ping the service IP it is a VIP it will not respond
3) If you defined a readiness probe make sure it is succeeding
4) To make sure a pod is part of a service by checking endpoints with "kubectl get endpoints"
5) If accessing it with FQDN ie. myservice.mynamespace.svc.cluster.local, try accessing with the clusterip
6) Make sure you are connecting on the exposed port and not the targetPort
7) Try connecting directly to the pod IP to make sure the pod is accepting connections
8) If you can not get to the app through the pod's IP make sure the app is not only binding to localhost


*  Volumes: attaching disk storage to containers
Volumes allows mutliple containers in a single pod to share storage
Many multiple types, we will start with emptydir


I built and pushed a container that will use the linux fortune command to write content to /var/htdocs/index.html
Here is the steps I took to build the container, first create folder and add this script to it
#+begin_example
#!/bin/bash
trap "exit" SIGINT
mkdir /var/htdocs
while :
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune > /var/htdocs/index.html
  sleep 10
done
#+end_example

Next I go add a docker file
#+begin_example
FROM ubuntu:latest
USER root
ADD fortuneloop.sh /bin/fortuneloop.sh
RUN apt-get update ; apt-get -y install fortune; chmod 755 /bin/fortuneloop.sh
ENTRYPOINT /bin/fortuneloop.sh
#+end_example
One thing to note when I first ran this it failed with permission denied for fortuneloop, so I add the USER root and chmod, not sure if it is correct, but it made sense

Finally I built and pushed with
#+begin_example
docker build -t bernokl/fortune .
docker push bernokl/fortune
#+end_example

Mkay lets take our new image and create the pod with the two shared mounts 
#+begin_src yaml :tangle yaml/fortune-pod.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: fortune
  spec:
    containers:
      - image: bernokl/fortune
        name: html-generator
        volumeMounts:
        - name: html
          mountPath: /var/htdocs
      - image: nginx:alpine
        name: web-server
        volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
          readOnly: true
        ports:
        - containerPort: 80
          protocol: TCP
    volumes:
    - name: html
      emptyDir: {}
#+end_src


The same html volume gets mounted on two different paths in each container
html-generator that way html-generator and web-server can share the same index.html
Lets apply it and see if it will run
#+begin_src shell :restults raw
kubectl apply -f yaml/fortune-pod.yaml 
#+end_src

#+RESULTS:
: pod/fortune created

#+begin_src shell :results raw
kubectl get po 
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE
dnsutils      1/1     Running   0          6h
fortune       2/2     Running   0          52s
kubia-dh8kt   1/1     Running   0          4d23h
kubia-nj8kd   1/1     Running   0          4d23h
kubia-zbk5r   1/1     Running   0          4d23h
NAME          READY   STATUS              RESTARTS   AGE
dnsutils      1/1     Running             0          5h59m
fortune       0/2     ContainerCreating   0          6s
kubia-dh8kt   1/1     Running             0          4d23h
kubia-nj8kd   1/1     Running             0          4d23h
kubia-zbk5r   1/1     Running             0          4d23h

OOO pretty, ok lets see if we can just forward the port and take a peek
I can not run this in here, it will hang out, gonna go try the following in an external shell just to forward the port
#+begin_example
kubectl port-forward fortune 8080:80 
#+end_example

and then lets try to curl it
#+begin_src shell :restults raw 
curl http://localhost:8080
#+end_src

#+RESULTS:
: Many changes of mind and mood; do not hesitate too long.

OO weirtd quote, still IT WORKED!!

YAY
I noticed something random fortune has the 2 containers, but they are not listed in the output above, I googled a bit to see how I can list them and there is some jasonpath foo that will do it
I did realize a simple cheat is to look for logs, the output will provide you with the names, the adult way is to describe the pod, that also lists containers.....
#+begin_src shell :results raw
kubectl logs fortune 
#+end_src

#+RESULTS:
a container name must be specified for pod fortune, choose one of: [html-generator web-server

not exactly correct, but it does the trick, again for more info run  "kubectl describe po fortune"
Good right?

** gitrepo volumes
K next we play with git based volumes, it is an extension of the emptyDir, it will create the emptyDir and then clone the specified repo into the directory

Lets build one to see what it looks like

#+begin_src yaml :tangle yaml/gitrepo-volume-pod.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: gitrepo-volume-pod
  spec:
    containers:
      - image: nginx:alpine
        name: web-server
        volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
          readOnly: true
        ports:
        - containerPort: 80
          protocol: TCP
    volumes:
    - name: html
      gitRepo:
        repository: https://github.com/luksa/kubia-website-example.git
        revision: master
        directory: .
#+end_src

#+begin_src shell :results raw
kubectl apply -f yaml/gitrepo-volume-pod.yaml
#+end_src

#+RESULTS:
pod/gitrepo-volume-pod created
pod "gitrepo-volume-pod" deleted
pod/gitrepo-volume-pod created

#+begin_src shell :results raw
kubectl get po 
#+end_src

#+RESULTS:
NAME                 READY   STATUS    RESTARTS   AGE
dnsutils             1/1     Running   0          11h
fortune              2/2     Running   0          5h14m
gitrepo-volume-pod   1/1     Running   0          38s
kubia-dh8kt          1/1     Running   0          5d4h
kubia-nj8kd          1/1     Running   0          5d5h
kubia-zbk5r          1/1     Running   0          5d4h

I am once again running the kubectl port-forward gitrepo-volume-pod 8080:80
#+begin_src shell :results raw
curl localhost:8080
#+end_src

#+RESULTS:
<html>
<body>
Hello there.
</body>
</html>

And that is the content of the html file residing in that repo, YAS!
Note that the files will not update when you make a commit to the repo, you have to kill and restart the pod for the new commits to show up
Note that gitRepo volume is not made to work with private github repos, rather you need to add a git sync sidecar container to clone and keep a private repo synced

** hostpath volumes
   This is persistant volumes, on a node
VERY IMPORTANT this persists volumes per node, so if the pod moves to a new node the volume will not move with it
in other words this is not the volume type you want for a database, hostpath volumes are not good for regular pods, it makes them rely on a specific node
A good use of this would be the system pods, ie daemonsets where you will have one of each pod on the node
Lets look at an example
#+begin_src shell :results raww
 kubectl get pods C-n kube-system
#+end_src

#+RESULTS:
| NAME                                      | READY | STATUS    | RESTARTS |  AGE |
| coredns-66bff467f8-96gv4                  | 1/1   | Running   |        2 | 102d |
| etcd-minikube                             | 1/1   | Running   |        1 |  26d |
| ingress-nginx-admission-create-fpcvv      | 0/1   | Completed |        0 | 102d |
| ingress-nginx-admission-patch-6qfcz       | 0/1   | Completed |        0 | 102d |
| ingress-nginx-controller-799dcd5d57-cmd68 | 1/1   | Running   |        1 |  26d |
| kube-apiserver-minikube                   | 1/1   | Running   |        2 | 102d |
| kube-controller-manager-minikube          | 1/1   | Running   |        2 | 102d |
| kube-proxy-d892r                          | 1/1   | Running   |        2 | 102d |
| kube-scheduler-minikube                   | 1/1   | Running   |        2 | 102d |
| registry-65xp4                            | 1/1   | Running   |        2 | 102d |
| registry-proxy-dqn2b                      | 1/1   | Running   |        2 | 102d |
| storage-provisioner                       | 1/1   | Running   |        8 | 102d |

#+begin_src shell :results raw
kubectl describe po etcd-minikub -n kube-system 
#+end_src

#+RESULTS:
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/minikube/certs/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/minikube/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         BestEffort
Node-Selectors:    <none>
Tolerations:       :NoExecute
Events:            <none>


This means host path volumes is only used to read system data, ie minikube certs in this case
For instance /var/log or other shared system volumes the pod needs, but that is specific per node

** Persistant storage
   This is volumes of data that persists regardless which node the pod comes up on
It would usually be comething like S3 o GCE
Kubernetes will provision disk on your cloud provider for you, but you can also do it manually
Here is an example creating the volume and then creating the pod
#+begin_example
$ gcloud container clusters list
        NAME   ZONE            MASTER_VERSION  MASTER_IP       ...
        kubia  europe-west1-b  1.2.5           104.155.84.137  ...
$ gcloud compute disks create --size=1GiB --zone=europe-west1-b mongodb
        NAME     ZONE            SIZE_GB  TYPE         STATUS
        mongodb  europe-west1-b  1        pd-standard  READY
#+end_example

Then you can mount the above created volume with the following yaml
#+begin_example yaml/mongodb-pod-gcepd.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mongodb
spec:
  volumes:
  - name: mongodb-data
    gcePersistentDisk:
      pdName: mongodb
      fsType: ext4
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
#+end_example
Things to note
- The type of the volume is a GCE Persistent Disk.
- The name of the persistent disk must match the actual PD you created earlier.
- The filesystem type is EXT4 (a type of Linux filesystem).
- Mountpath is The path where MongoDB stores its data
- The name of the volume (also referenced when mounting the volume)

k now lets try using a persistent volume on my machines /tmp to see if minikube will mount that

#+begin_src yaml :tangle yaml/mongodb-pod-hostpath.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: mongodb
  spec:
    volumes:
    - name: mongodb-data
      hostPath:
        path: /tmp/mongodb
    containers:
    - image: mongo
      name: mongodb
      volumeMounts:
      - name: mongodb-data
        mountPath: /data/db
      ports:
      - containerPort: 27017
        protocol: TCP
#+end_src

First I want to confirm the tmp dir does not exist
#+begin_src shell :results raw
ls -al /tmp/mongodb 
#+end_src

#+RESULTS:
 /tmp/mongodb: No such file or directory

k lets go apply the pod and see what happens
Ugg bugger that is a hostpath so specific to the node, so still only inside minikube, I need to see how I persist that out to outside of minikube
#+begin_src shell :results raw
kubectl apply -f yaml/mongodb-pod-hostpath.yaml 
#+end_src

#+RESULTS:
pod/mongodb created

#+begin_src shell :results raw
kubectl get po
#+end_src

#+RESULTS:
NAME                 READY   STATUS              RESTARTS   AGE
dnsutils             1/1     Running             0          13h
fortune              2/2     Running             0          7h7m
gitrepo-volume-pod   1/1     Running             0          108m
kubia-dh8kt          1/1     Running             0          5d6h
kubia-nj8kd          1/1     Running             0          5d7h
kubia-zbk5r          1/1     Running             0          5d6h
mongodb              0/1     ContainerCreating   0          3s

#+begin_src shell :results raw
ls -al /tmp/mongodb
#+end_src

#+RESULTS:
 /tmp/mongodb: No such file or directory
 

Yip confirmed it is /tmp inside the minikube vm....
#+begin_example
$ minikube ssh

$ ls -al /tmp/mongodb
total 296
drwxr-xr-x 4  999 root   400 Nov 30 11:12 .
drwxrwxrwt 7 root root   180 Nov 30 11:13 ..
-rw------- 1  999  999    47 Nov 30 10:57 WiredTiger
-rw------- 1  999  999    21 Nov 30 10:57 WiredTiger.lock
-rw------- 1  999  999  1247 Nov 30 11:12 WiredTiger.turtle
#+end_example

There is lots more info on creating persistant cloud storage, there is examples using AWS elastic blocks and even 
Microsoft Azure, you can use the azureFile or the azureDisk. They all follow the same format. I am using minikube so I am not going further into it

Dont forget to use kubectl explain to get more info on anything you encounter

** persistant Volume
So next discussion is that you do not really want to set up your volumes as part of a pod, it limits where it runs
Instead we have the cluster admin set up a persistant volume, the pod will then just have a volumeClaim
That is agnostic request for storage without worrying about what is connected.

Lets go set up a PersistantVolume

#+begin_src yaml :tangle yaml/mongodb-pv-hostpath.yaml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: mongodb-pv
  spec:
    capacity:
      storage: 1Gi
    accessModes:
      - ReadWriteOnce
      - ReadOnlyMany
    persistentVolumeReclaimPolicy: Retain
    hostPath:
      path: /tmp/mongodb
#+end_src

This is an example of what that volume would look like if we were doing it with cloud storage
Basically the last couple of lines change to point to GCE instead of hostpath
#+begin_example
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  gcePersistentDisk:
    pdName: mongodb
    fsType: ext4
#+end_example

K, pv is tangled lets go apply it and see what hapens
#+begin_src shell :results raw
kubectl apply -f yaml/mongodb-pv-hostpath.yaml 
#+end_src

#+RESULTS:
persistentvolume/mongodb-pv created

#+begin_src shell :results raw
kubectl get pv 
#+end_src

#+RESULTS:
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
mongodb-pv   1Gi        RWO,ROX        Retain           Available                                   31s

Next we are going to put in a persistentVolumeClaim, pv is not limited to a namespace, pvc is used to assign a volume to a namespace
you will then use that pvc name in your pods, this allows them to have volumes accross nodes

Lets go put in out PVC

#+begin_src yaml :tangle yaml/mongodb-pvc.yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: mongodb-pvc
  spec:
    resources:
      requests:
        storage: 1Gi
    accessModes:
    - ReadWriteOnce
    storageClassName: ""
#+end_src
Things to note:
- The name of your claim (under metada)—you’ll need this later when using the claim as the pod’s volume.
- Size gets defined in here
- For thsi claim You want the storage to support a single client (performing both reads and writes)
- The open quotes on storageclassName means it is dynamic provisioning

#+begin_src shell :results raw
kubectl apply -f yaml/mongodb-pvc.yaml 
#+end_src

#+RESULTS:
persistentvolumeclaim/mongodb-pvc created

#+begin_src shell :results raw
kubectl get pvc
#+end_src

#+RESULTS:
NAME          STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mongodb-pvc   Bound    mongodb-pv   1Gi        RWO,ROX                       25s

#+begin_src shell :results raw
kubectl get pv 
#+end_src

#+RESULTS:
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
mongodb-pv   1Gi        RWO,ROX        Retain           Bound    default/mongodb-pvc                           24m

Nice both pvc and pv sees it as bound, also note I did not tell pvc anything about the pv k8s looked for available storage and bound the 2
Note that ReadWriteOnce pertains to the number of nodes that can attatch not pods, so in this case only one node can attach to the volume
Also note that the CLAIM includes the namesapce, default in this case

Aite we have a pvc, lets see if we can add it to our pod, I deleted mongodb earlier, lets confirm

#+begin_src shell :results raw
kubectl get po 
#+end_src

#+RESULTS:
NAME                 READY   STATUS    RESTARTS   AGE
dnsutils             1/1     Running   0          25h
fortune              2/2     Running   0          19h
gitrepo-volume-pod   1/1     Running   0          13h
kubia-dh8kt          1/1     Running   0          5d18h
kubia-nj8kd          1/1     Running   0          5d18h
kubia-zbk5r          1/1     Running   0          5d18h

Looks good to me
#+begin_src yaml :tangle yaml/mongodb-pod-pvc.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: mongodb
  spec:
    containers:
    - image: mongo
      name: mongodb
      volumeMounts:
      - name: mongodb-data
        mountPath: /data/db
      ports:
      - containerPort: 27017
        protocol: TCP
    volumes:
    - name: mongodb-data
      persistentVolumeClaim:
        claimName: mongodb-pvc
#+end_src

Lets see what we did
#+begin_src shell :results raw 
kubectl apply -f yaml/mongodb-pod-pvc.yaml
#+end_src

#+RESULTS:
pod/mongodb created

Yeah buddy, that is it, now we have a PV tied to a PVC that the pod can use when defining volumes.... SOOO Goood

Now as previously mentioned that kubernetes can automatically provision the PV it is built in for a few cloud providers, 
but for on prem or other custom storage a provisioner needs to be deployed in the cluster to allow it to auto provision the storage.
To be able to allow this kind of provisioning the cluster-admin needs to provision storageClass that will define the types of storage available.

This is an example GCE fast storageClass definition
#+begin_example
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
  zone: europe-west1-b
#+end_example
Things to note
- We define the provisioner as the volume plugin that will be used kubernetes.io/gce-pd in our example
- The parameters is what we pass to the provisioner to define the type of storage

Lets see what the same thing looks like for minikube
#+begin_src yaml :tangle yaml/storageclass-fast-hostpath.yaml
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    name: fast
  provisioner: k8s.io/minikube-hostpath
  parameters:
    type: pd-ssd
#+end_src

#+begin_src shell :results raw 
kubectl apply -f yaml/storageclass-fast-hostpath.yaml
#+end_src

#+RESULTS:
storageclass.storage.k8s.io/fast created

#+begin_src shell :results raw 
kubectl get storageclass
#+end_src

#+RESULTS:
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
fast                 k8s.io/minikube-hostpath   Delete          Immediate           false                  28s
standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  103d

OK lets go see if can create a pvc with dynamic provisioning

#+begin_src yaml :tangle yaml/mongodb-pvc-dp.yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: mogodb-pvc-dp
  spec:
    storageClassName: fast
    resources:
      requests:
        storage: 100Mi
    accessModes:
      - ReadWriteOnce
#+end_src

#+begin_src shell :results raw
kubectl apply -f yaml/mongodb-pvc-dp.yaml 
#+end_src

#+RESULTS:
persistentvolumeclaim/mogodb-pvc-dp created
persistentvolumeclaim "mogodb-pvc" deleted
persistentvolumeclaim/mogodb-pvc created

#+begin_src shell :results raw
kubectl get pvc
#+end_src

#+RESULTS:
NAME            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mogodb-pvc-dp   Bound    pvc-020cdd8b-af12-4f4d-95f5-66a4c5db491f   100Mi      RWO            fast           17s
mongodb-pvc     Bound    mongodb-pv                                 1Gi        RWO,ROX                       4h6m

#+begin_src shell :results raw
kubectl get pv
#+end_src

#+RESULTS:
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS   REASON   AGE
mongodb-pv                                 1Gi        RWO,ROX        Retain           Bound    default/mongodb-pvc                             4h31m
pvc-020cdd8b-af12-4f4d-95f5-66a4c5db491f   100Mi      RWO            Delete           Bound    default/mogodb-pvc-dp   fast                    68s



It is interesting that there is a default storageclass even before I created one
#+begin_src shell :results raw
kubectl get sc 
#+end_src

#+RESULTS:
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
fast                 k8s.io/minikube-hostpath   Delete          Immediate           false                  15h
standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  103d

Lets get a bit more detail on that standard sc
#+begin_src shell :results raw 
kubectl get sc standard -o yaml 
#+end_src

#+RESULTS:
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
    storageclass.kubernetes.io/is-default-class: "true"
  creationTimestamp: "2020-08-19T22:14:48Z"
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
  managedFields:
  - apiVersion: storage.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:kubectl.kubernetes.io/last-applied-configuration: {}
          f:storageclass.kubernetes.io/is-default-class: {}
        f:labels:
          .: {}
          f:addonmanager.kubernetes.io/mode: {}
      f:provisioner: {}
      f:reclaimPolicy: {}
      f:volumeBindingMode: {}
    manager: kubectl
    operation: Update
    time: "2020-08-19T22:14:48Z"
  name: standard
  resourceVersion: "250"
  selfLink: /apis/storage.k8s.io/v1/storageclasses/standard
  uid: 822d13cc-5e04-4f61-9b56-ecb487aa62a3
provisioner: k8s.io/minikube-hostpath
reclaimPolicy: Delete
volumeBindingMode: Immediate

This means that I could have provisoned a PVC without defining a PV, lets test lets create pvc without a storageClassName specified
#+begin_src yaml :tangle yaml/mongodb-pvc-dp-nostorageclass.yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata: 
    name: mongodb-pvc2
  spec:
    resources:
      requests:
        storage: 100Mi
    accessModes:
      - ReadWriteOnce
#+end_src

#+begin_src shell :results raw
kubectl apply -f yaml/mongodb-pvc-dp-nostorageclass.yaml
#+end_src

#+RESULTS:
persistentvolumeclaim/mongodb-pvc2 created

#+begin_src shell :results raw
kubectl get pvc mongodb-pvc2
#+end_src

#+RESULTS:
NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mongodb-pvc2   Bound    pvc-0ff2103f-8740-48c8-baac-a6226fd6ca9c   100Mi      RWO            standard       26s

#+begin_src shell :results raw
kubectl get pv pvc-0ff2103f-8740-48c8-baac-a6226fd6ca9c
#+end_src

#+RESULTS:
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE
pvc-0ff2103f-8740-48c8-baac-a6226fd6ca9c   100Mi      RWO            Delete           Bound    default/mongodb-pvc2   standard                68s

This is important to note,
Specifying an empty string as the storage class name ensures the PVC binds to a pre-provisioned PV instead of dynamically provisioning a new one.
#+begin_example
 kind: PersistentVolumeClaim
spec:
  storageClassName: ""
#+end_example

* ConfigMaps and secrets
You can configure your apps by
- Passing command-line arguments to containers
- Setting custom environment variables for each container
- Mounting configuration files into containers through a special type of volume

In a Dockerfile, two instructions define the commands to run and the arguments:
- ENTRYPOINT defines the executable invoked when the container is started.
- CMD specifies the arguments that get passed to the ENTRYPOINT.

Both of these support different forms:
- shell form—For example, ENTRYPOINT node app.js.
- exec form—For example, ENTRYPOINT ["node", "app.js"].

Shell means it will be ran inside a shell command, so the ps of the shell format would
first have a shell process that spawns the node process
#+begin_example
$ docker exec -it e4bad ps x
  PID TTY      STAT   TIME COMMAND
    1 ?        Ss     0:00 /bin/sh -c node app.js
    7 ?        Sl     0:00 node app.js
#+end_example

The exec form will directly run node (not inside a shell)
#+begin_example
$ docker exec 4675d ps x
    PID TTY  STAT   TIME COMMAND
    1    ?   Ssl    0:00 node app.js
#+end_example


Ok lets start with a simple configurable image so we can play, I am going to update the forune script to allow the interval to be configuratble

#+begin_example
#!/bin/bash
trap "exit" SIGINT
INTERVAL=$1
echo Configured to generate new fortune every $INTERVAL seconds
mkdir /var/htdocs
while :
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune > /var/htdocs/index.html
  sleep $INTERVAL
done
#+end_example

Now lets change the Dockerfile to use the exec method so we can pass it some the vars
#+begin_example
FROM ubuntu:latest
RUN apt-get update ; apt-get -y install fortune
ADD fortuneloop.sh /bin/fortuneloop.sh
ENTRYPOINT ["/bin/fortuneloop.sh"]
CMD ["10"]
#+end_example

k, now lets build and push
#+begin_example
$ docker build -t docker.io/bernokl/fortune:args .
$ docker push docker.io/bernokl/fortune:args
#+end_example

I can now see my new var being used with
#+begin_example
$docker run -it docker.io/bernokl/fortune:args
  Configured to generate new fortune every 10 seconds
  Tue Dec 1 22:21:21 UTC 2020 Writing fortune to /var/htdocs/index.html
#+end_example

Lets pass in a new value for our arg
#+begin_example
$docker run -it docker.io/bernokl/fortune:args 16
  Configured to generate new fortune every 16 seconds
  Tue Dec 1 22:21:55 UTC 2020 Writing fortune to /var/htdocs/index.html
#+end_example

HOW cool is that?

In a pod we can overide the ENTRYPOINT and CMD with command and args, it will look something like this
#+begin_example
kind: Pod
  spec:
  containers:
  - image: some/image
    command: ["/bin/command"]
    args: ["arg1", "arg2", "arg3"]
#+end_example

Lets combine all this amazing knowledge to build a new fortune pod that will use the arguments

#+begin_src yaml :tangle yaml/fortune-pod-args.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: fortune2s
  spec:
    containers:
    - image: bernokl/fortune:args
      imagePullPolicy: Always
      args: ["2"]
      name: html-generator
      volumeMounts:
      - name: html
        mountPath: /var/htdocs
    - image: nginx:alpine
      name: web-server
      volumeMounts:
      - name: html
        mountPath: /usr/share/nginx/html
        readOnly: true
      ports:
      - containerPort: 80
        protocol: TCP
    volumes:
    - name: html
      emptyDir: {}
#+end_src

#+begin_src shell :results raw
kubectl apply -f yaml/fortune-pod-args.yaml 
#+end_src

#+RESULTS:
pod/fortune2s created
pod "fortune2s" deleted
pod/fortune2s created
pod "fortune2s" deleted
pod/fortune2s created

#+begin_src shell :results raw
kubectl get po
#+end_src

#+RESULTS:
NAME                 READY   STATUS    RESTARTS   AGE
dnsutils             1/1     Running   0          2d
fortune              2/2     Running   0          42h
fortune2s            2/2     Running   0          19s
gitrepo-volume-pod   1/1     Running   0          37h
kubia-dh8kt          1/1     Running   0          6d18h
kubia-nj8kd          1/1     Running   0          6d18h
kubia-zbk5r          1/1     Running   0          6d18h
mongodb              1/1     Running   0          23h

#+begin_src shell :results raw
kubectl logs fortune2s -c html-generator | tail 
#+end_src

#+RESULTS:
Configured to generate new fortune every 2 seconds
Tue Dec 1 23:25:39 UTC 2020 Writing fortune to /var/htdocs/index.html
Configured to generate new fortune every 2 seconds
Tue Dec 1 23:25:41 UTC 2020 Writing fortune to /var/htdocs/index.html
Configured to generate new fortune every 2 seconds
Tue Dec 1 23:25:43 UTC 2020 Writing fortune to /var/htdocs/index.html
Configured to generate new fortune every 2 seconds
Tue Dec 1 23:25:45 UTC 2020 Writing fortune to /var/htdocs/index.html
Configured to generate new fortune every 2 seconds
Tue Dec 1 23:25:47 UTC 2020 Writing fortune to /var/htdocs/index.html

Can I have a whoop whoop

got to 7.3 Setting environment variables for a container
