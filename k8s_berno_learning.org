# Just capturing some ideas

* Lets test functionalit
- Create code block with ,ibs type shell and add your command, 
- To run the command navigate into the block in normal mode and hit ,,
#+begin_src shell
kubectl get svc -A
#+end_src

#+RESULTS:
| NAMESPACE            | NAME                               | TYPE         |     CLUSTER-IP | EXTERNAL-IP | PORT(S)                |  AGE |
| default              | kubernetes                         | ClusterIP    |      10.96.0.1 | <none>      | 443/TCP                |  82d |
| default              | kubia-http                         | LoadBalancer | 10.108.152.157 | <pending>   | 8080:31470/TCP         | 6d8h |
| kube-system          | ingress-nginx-controller-admission | ClusterIP    |  10.99.132.235 | <none>      | 443/TCP                |  82d |
| kube-system          | kube-dns                           | ClusterIP    |     10.96.0.10 | <none>      | 53/UDP,53/TCP,9153/TCP |  82d |
| kube-system          | registry                           | ClusterIP    | 10.100.221.151 | <none>      | 80/TCP,443/TCP         |  82d |
| kubernetes-dashboard | dashboard-metrics-scraper          | ClusterIP    |   10.109.68.77 | <none>      | 8000/TCP               | 6d7h |
| kubernetes-dashboard | kubernetes-dashboard               | ClusterIP    |  10.100.239.36 | <none>      | 80/TCP                 | 6d7h |

ok lets go with the lesson, starting on page 61 3.2 Creating pods from YAML....

#+begin_src shell
kubectl get pods -n default 
#+end_src

#+RESULTS:
| NAME        | READY | STATUS  | RESTARTS | AGE  |
| kubia-4lp4r | 1/1   | Running |        0 | 6d8h |

#+begin_src shell
kubectl get po kubia-4lp4r -o yaml 
#+end_src

I got to introducint the main parts of a pod definition on the bottom or page 62 tbc......

* Pod yaml anatomy
- version of api / kind of resource
- Metadata: Name, namespace, labels and other
- spec: contains the actual description of the pos'd contents, such as the pod's containers, volumes, and other data
- Status: This is only for running container, contains ip, status etc

* lets create a YAML descriptor for a pod
I would like to create the file here and then tangle it out, lets try
#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/kubia-manual.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual
spec:
  containers:
  - image: bernokl/kubia
    name: kubia
    ports:
    - containerPort: 8080
      protocol: TCP
#+end_src

Yay tangle ended up being simple, key is header, to generate file hit ,bt
Read more about it on [Babel Intro](https://orgmode.org/worg/org-contrib/babel/intro.html) 
or [examples](https://orgmode.org/manual/Literal-Examples.html)

#+begin_example
  "#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/kubia-manual.yaml"
#+end_example

Lets apply it
#+begin_src shell
kubectl create -f /Users/bernokl/projects/k8sInAction/yaml/kubia-manual.yaml
#+end_src

#+RESULTS:
: pod/kubia-manual created

Lets go look at the new pod
#+begin_src shell
kubectl get pods -n default 
#+end_src

#+RESULTS:
| NAME         | READY | STATUS  | RESTARTS | AGE   |
| kubia-4lp4r  | 1/1   | Running |        0 | 7d8h  |
| kubia-manual | 1/1   | Running |        0 | 3m57s |

Lets look at the logs
#+begin_src shell
 kubectl logs kubia-manual 
#+end_src

#+RESULTS:
: Kubia server starting...

Nice hint if you have multiple containers in a pod you can point to the specific container with -c
#+begin_example
  kubectl logs kubia-manual -c kubia
#+end_example

Note logs die with pods unless centralized logging is configured.

Lets forward the port so we can get to it
(Dont run this, it is interactive and will hang, 
if it does "ps faux | grep port-forward" run in a shell outside emacs should help you locate the pid) 
#+begin_src xx shell
  kubectl port-forward kubia-manual 8888:8080 
#+end_src

The avove is really handy it gives you access from your local network to a pod
#+begin_src shell :results value verbatim
  #curl localhost:8080 
  ps aux | grep port | grep kubia | grep -v grep
#+end_src

#+RESULTS:
: bernokl          54743   0.0  0.1  4445792  28704 s006  S+    7:43AM   0:01.51 kubectl port-forward kubia-manual 8888:8080

* Lets create pods with labels
  
#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/kubia-manual-with-Labels.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: kubia-manual-v2
    labels:
      creation_method: manual
      env: prod
  spec:
    containers:
    - image: bernokl/kubia
      name: kubia
      ports:
      - containerPort: 8080
        protocol: TCP
#+end_src

Tangled  the above out, just for nerdness lets confirm
#+begin_src shell :results value verbatim
head  /Users/bernokl/projects/k8sInAction/yaml/kubia-manual-with-Labels.yaml
#+end_src

#+RESULTS:
#+begin_example
apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual-v2
  labels:
    creation_method: manual
    env: prod
spec:
  containers:
  - image: bernokl/kubia
#+end_example


OK, lets go apply that sucker
#+begin_src shell
kubectl create -f yaml/kubia-manual-with-labels.yaml
#+end_src

#+RESULTS:
: pod/kubia-manual-v2 created

Should we have 3 pods now?
#+begin_src shell :results value verbatim
kubectl get pods -n default 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE
: kubia-4lp4r       1/1     Running   0          7d18h
: kubia-manual      1/1     Running   0          10h
: kubia-manual-v2   1/1     Running   0          76s

Whoop, k lets go look at the labels
#+begin_src shell :results value verbatim
kubectl get po --show-labels -n default 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE     LABELS
: kubia-4lp4r       1/1     Running   0          7d18h   app=kubia
: kubia-manual      1/1     Running   0          10h     <none>
: kubia-manual-v2   1/1     Running   0          2m28s   creation_method=manual,env=prod

It is also possibel to organize the layout for label values
#+begin_src shell :results value verbatim
kubectl get pods -L creation_method,env 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE     CREATION_METHOD   ENV
: kubia-4lp4r       1/1     Running   0          7d19h                     
: kubia-manual      1/1     Running   0          10h                       
: kubia-manual-v2   1/1     Running   0          5m29s   manual            prod

Lets go update labels of our running pod
#+begin_src shell :results value verbatim
kubectl label po kubia-manual creation_method=manual 
#+end_src

#+RESULTS:
: pod/kubia-manual labeled

ok lets go see what that label looks like
#+begin_src shell :results value verbatim
kubectl get pods -L creation_method,env 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE     CREATION_METHOD   ENV
: kubia-4lp4r       1/1     Running   0          7d19h                     
: kubia-manual      1/1     Running   0          10h     manual            
: kubia-manual-v2   1/1     Running   0          24s     manual            prod

Nice!

Lets go overwrite an existing label say change env to prod for v2
#+begin_src shell :results value verbatim
kubectl label po kubia-manual-v2 env=debug --overwrite 
#+end_src

#+RESULTS:
: pod/kubia-manual-v2 labeled

ok lets go see what that label looks like
#+begin_src shell :results value verbatim
kubectl get pods -L creation_method,env 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE     CREATION_METHOD   ENV
: kubia-4lp4r       1/1     Running   0          7d19h                     
: kubia-manual      1/1     Running   0          10h     manual            
: kubia-manual-v2   1/1     Running   0          45s     manual            debug

More playing with labels
Lets just list pods that were manually created
#+begin_src shell :results value verbatim
kubectl get po -l creation_method=manual 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE
: kubia-manual      1/1     Running   0          10h
: kubia-manual-v2   1/1     Running   0          9m37s

Lets get all pods with an env labels
#+begin_src shell :results value verbatim
kubectl get po -l env 
#+end_src

#+RESULTS:
: NAME              READY   STATUS    RESTARTS   AGE
: kubia-manual-v2   1/1     Running   0          10m

Lets get all pods without env labels
#+begin_src shell :results value verbatim
kubectl get po -l '!env',creation_method!=manual
#+end_src

#+RESULTS:
: NAME          READY   STATUS    RESTARTS   AGE
: kubia-4lp4r   1/1     Running   0          7d19h

That is pretty sweet I added a second condition that just worked looks like it could be very powerful

One last thing lets label our node so we can organize by those labels
#+begin_src shell :results value verbatim 
  kubectl get node
  kubectl label node minikube fun=true
  kubectl get nodes -l fun=true
  kubectl get nodes -L fun=true
#+end_src

#+RESULTS:
: NAME       STATUS   ROLES    AGE   VERSION
: minikube   Ready    master   83d   v1.18.3

: NAME       STATUS   ROLES    AGE   VERSION
: minikube   Ready    master   83d   v1.18.3

: NAME       STATUS   ROLES    AGE   VERSION   FUN=TRUE
: minikube   Ready    master   83d   v1.18.3   

* Schedule pods to specific nodes

#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/kubia-fun.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: kubia-fun
  spec:
    nodeSelector:
      fun: "true"
    containers:
    - image: bernokl/kubia
      name: kubia
#+end_src

lets go apply it, this will only allow my kibia fun pod to run on a node that has fun as true
In my case I am using minikube so I only have one node, but the idea is amazing
Note that it is nodeSelector above that ties my pod to that node
#+begin_src shell  
kubectl create -f yaml/kubia-fun.yaml
#+end_src

#+RESULTS:
: pod/kubia-fun created

#+begin_src shell :results value verbatim 
kubectl get nodes --show-labels
#+end_src 

#+RESULTS:
: NAME       STATUS   ROLES    AGE   VERSION   LABELS
: minikube   Ready    master   83d   v1.18.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,fun=true,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,minikube.k8s.io/commit=2243b4b97c131e3244c5f014faedca0d846599f5,minikube.k8s.io/name=minikube,minikube.k8s.io/updated_at=2020_08_20T10_14_48_0700,minikube.k8s.io/version =v1.12.3,node-role.kubernetes.io/master=


#+begin_src bash
kubectl get po -n default
#+end_src

#+RESULTS:
| NAME            | READY | STATUS  | RESTARTS | AGE   |
| kubia-4lp4r     | 1/1   | Running |        0 | 7d20h |
| kubia-fun       | 1/1   | Running |        0 | 23m   |
| kubia-manual    | 1/1   | Running |        0 | 11h   |
| kubia-manual-v2 | 1/1   | Running |        0 | 61m   |

 
* Anotations
  Very handy, we can use them to add metadata, they are not meant to be used to sort on like labels, but rather provide info
#+begin_src shell :results value verbatim 
  kubectl get po kubia-manual -o yaml > tmp
  head tmp
  rm tmp
#+end_src

#+RESULTS:
#+begin_example
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2020-11-11T09:54:20Z"
  labels:
    creation_method: manual
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
#+end_example

Ok the guy above has no annotations, lets add one
#+begin_src shell :results value verbatim
kubectl annotate pod kubia-manual bernokl.com/letsaddanotation="interesting facts" 
#+end_src

#+RESULTS:
: pod/kubia-manual annotated


#+begin_src shell :results value verbatim 
  kubectl get po kubia-manual -o yaml > tmp
  head tmp
  rm tmp
#+end_src

#+RESULTS:
#+begin_example
apiVersion: v1
kind: Pod
metadata:
  annotations:
    bernokl.com/letsaddanotation: interesting facts
  creationTimestamp: "2020-11-11T09:54:20Z"
  labels:
    creation_method: manual
  managedFields:
  - apiVersion: v1
#+end_example
 
Lets describe it
#+begin_src shell :results verbatim 
kubectl describe pod kubia-manual
#+end_src

#+RESULTS:
#+begin_example
Name:         kubia-manual
Namespace:    default
Priority:     0
Node:         minikube/192.168.64.2
Start Time:   Wed, 11 Nov 2020 22:54:20 +1300
Labels:       creation_method=manual
Annotations:  bernokl.com/letsaddanotation: interesting facts
Status:       Running
IP:           172.17.0.9
IPs:
  IP:  172.17.0.9
Containers:
  kubia:
    Container ID:   docker://f2febbb9016aeb839478ce1869b88dfa8228d3b4ec5edf646898bd5d9fb21d2a
    Image:          bernokl/kubia
    Image ID:       docker-pullable://bernokl/kubia@sha256:13f94084a4515abf331b5a9e751e355964624124c52755a6589abde31afd5c64
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 11 Nov 2020 22:54:27 +1300
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xnktg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-xnktg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-xnktg
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>
#+end_example


* Name spaces
Lets see what namespaces we have
#+begin_src shell :results verbatim
kubectl get ns 
#+end_src

#+RESULTS:
: NAME                   STATUS   AGE
: default                Active   84d
: kube-node-lease        Active   84d
: kube-public            Active   84d
: kube-system            Active   84d
: kubernetes-dashboard   Active   7d20h

Lets look to see what we can find in kube-system
#+begin_src shell :results verbatim 
kubectl get po --namespace kube-system
#+end_src

#+RESULTS:
#+begin_example
NAME                                        READY   STATUS      RESTARTS   AGE
coredns-66bff467f8-96gv4                    1/1     Running     1          84d
etcd-minikube                               1/1     Running     0          8d
ingress-nginx-admission-create-fpcvv        0/1     Completed   0          84d
ingress-nginx-admission-patch-6qfcz         0/1     Completed   0          84d
ingress-nginx-controller-799dcd5d57-cmd68   1/1     Running     0          8d
kube-apiserver-minikube                     1/1     Running     1          84d
kube-controller-manager-minikube            1/1     Running     1          84d
kube-proxy-d892r                            1/1     Running     1          84d
kube-scheduler-minikube                     1/1     Running     1          84d
registry-65xp4                              1/1     Running     1          84d
registry-proxy-dqn2b                        1/1     Running     1          84d
storage-provisioner                         1/1     Running     5          84d
#+end_example

Lets create a new namespace using a yaml file
#+begin_src yaml :tangle yaml/custom-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: custom-namespace
#+end_src

Two easy ways to create it, run the file above or even better just run `kubectl create namespace`
#+begin_src shell :results raw
  #kubectl create -f yaml/custom-namespace.yaml
  kubectl delete namespace custom-namespace2
#+end_src

#+RESULTS:
namespace "custom-namespace2" deleted
namespace/custom-namespace2 created
namespace/custom-namespace created

NO DOTS IN NAMESPACE NAMES

To create someting in ns simply add namespace: to the metadata of the pod.yaml or identify it in line
#+begin_src shell :results raw
kubectl create -f yaml/kubia-manual.yaml -n custom-namespace 
#+end_src

#+RESULTS:
pod/kubia-manual created

#+begin_src shell :results raw
kubectl get pods -A
#+end_src

#+RESULTS:
NAMESPACE              NAME                                        READY   STATUS      RESTARTS   AGE
custom-namespace       kubia-manual                                1/1     Running     0          2m11s
default                kubia-4lp4r                                 1/1     Running     0          7d21h
default                kubia-fun                                   1/1     Running     0          125m
default                kubia-manual                                1/1     Running     0          13h
default                kubia-manual-v2                             1/1     Running     0          163m
kube-system            coredns-66bff467f8-96gv4                    1/1     Running     1          84d
kube-system            etcd-minikube                               1/1     Running     0          8d
kube-system            ingress-nginx-admission-create-fpcvv        0/1     Completed   0          84d
etc.......

#+begin_src shell
kubectl get pods 
#+end_src

#+RESULTS:
| NAME            | READY | STATUS  | RESTARTS | AGE   |
| kubia-4lp4r     | 1/1   | Running |        0 | 7d22h |
| kubia-fun       | 1/1   | Running |        0 | 135m  |
| kubia-manual    | 1/1   | Running |        0 | 13h   |
| kubia-manual-v2 | 1/1   | Running |        0 | 173m  |

Looks like my ns if I do not declare it is set to default, lets try to change it to custom-namespace
#+begin_src shell 
#kubectl config set-context --current --namespace=custom-namespace
kubectl config set-context --current --namespace=default
#+end_src

#+RESULTS:
: minikube

#+begin_src shell
kubectl get pods 
#+end_src

#+RESULTS:
| NAME         | READY | STATUS  | RESTARTS | AGE |
| kubia-manual | 1/1   | Running |        0 | 16m |

yay, makes sense, easy isolation for resources.

ok lets clean up
#+begin_src shell
kubectl delete po kubia-4lp4r
#+end_src

#+RESULTS:
: kubia-4lp4r

Lets delete by labels
#+begin_src shell
kubectl delete po -l creation_method=manual 
#+end_src

#+RESULTS:
| pod | kubia-manual    | deleted |
| pod | kubia-manual-v2 | deleted |

Lets delete the entire namespace
#+begin_src shell
kubectl delete ns custom-namespace 
#+end_src

#+RESULTS:
: custom-namespace

OK at this point I should just have the one pod remaining in default, lets confirm
#+begin_src shell
kubectl get po -A 
#+end_src

#+RESULTS:
| NAMESPACE            | NAME                                      | READY | STATUS    | RESTARTS |   AGE |
| default              | kubia-8hvvl                               | 1/1   | Running   |        0 | 5m10s |
| default              | kubia-fun                                 | 1/1   | Running   |        0 |  151m |
| kube-system          | coredns-66bff467f8-96gv4                  | 1/1   | Running   |        1 |   84d |
| kube-system          | etcd-minikube                             | 1/1   | Running   |        0 |    8d |
etc......

ok forgot about the one we tested nodeSelector on, k lets delete all pods in default see how we go
 #+begin_src shell
kubectl delete po --all -ndefault 
 #+end_src

 #+RESULTS:
 | pod | kubia-8hvvl | deleted |
 | pod | kubia-fun   | deleted |

 Lets confirm
#+begin_src shell
kubectl get po -A 
#+end_src

#+RESULTS:
| NAMESPACE            | NAME                                      | READY | STATUS    | RESTARTS |   AGE |
| default              | kubia-cj54r                               | 1/1   | Running   |        0 |   59s |
| kube-system          | coredns-66bff467f8-96gv4                  | 1/1   | Running   |        1 |   84d |
etc.....

O wait, kubia-cj54r is back, that is right it is a service so even if I delete the pod it will be back
This command very much seems like rm -rf to me, but you can delete most everything in a ns in one neat command
#+begin_src shell
kubectl delete all --all -n default
#+end_src

#+RESULTS:
| pod                   | kubia-cj54r | deleted |
| replicationcontroller | kubia       | deleted |
| service               | kubernetes  | deleted |
| service               | kubia-http  | deleted |

Nice it cleaned up replication controller and services I forgot about fs

* Pod health LIVENESS PROBES
3 types of livliness probes
- http get, performs GET request
- TCP socket probe, tries to bind to the specified port of the container
- Exec, executes an arbitrary command inside the container

To test http livliness probe use an image that has the nodjsApp report 500 after every 5th reques 
Lets build the yaml to create this pod

#+begin_src yaml :tangle yaml/kubia-liveness-probe.yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: kubia-liveness
  spec:
    containers:
    - image: luksa/kubia-unhealthy
      name: kubia
      livenessProbe:
        httpGet:
          path: /
          port: 8080
#+end_src

lets go apply it
#+begin_src shell :results raw
kubectl apply -f yaml/kubia-liveness-probe.yaml 
#+end_src

#+RESULTS:
pod/kubia-liveness created

#+begin_src shell results:raw
kubectl get po kubia-liveness
#+end_src

#+RESULTS:
| NAME           | READY | STATUS  | RESTARTS | AGE   |
| kubia-liveness | 1/1   | Running |        2 | 4m44s |

You can look at the logs for the pod that already crashed by using `--previous`
#+begin_src shell
kubectl logs kubia-liveness --previous 
#+end_src

#+RESULTS:
| Kubia    | server  | starting... |                   |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |
| Received | request | from        | ::ffff:172.17.0.1 |

I still think the best info on a container is in describe, Events is full of gold
#+begin_src shell :results raw
  kubectl describe po kubia-liveness > tmp
  head -15 tmp
  echo "----------------------------------"
  grep -i -b2 -a2 "last state" tmp
  echo "----------------------------------"
  grep -i -b2 -a2 "liveness" tmp
  echo "----------------------------------"
  tail -10 tmp
  rm tmp 
#+end_src

#+RESULTS:
Name:         kubia-liveness
Namespace:    default
Priority:     0
Node:         minikube/192.168.64.2
Start Time:   Thu, 12 Nov 2020 15:24:43 +1300
Labels:       <none>
Annotations:  Status:  Running
IP:           172.17.0.6
IPs:
  IP:  172.17.0.6
Containers:
  kubia:
    Container ID:   docker://d130e21b7472b362d744e5f8ec80439bcddecd435040ef3312477db2299f81a9
    Image:          luksa/kubia-unhealthy
    Image ID:       docker-pullable://luksa/kubia-unhealthy@sha256:5c746a42612be61209417d913030d97555cff0b8225092908c57634ad7c235f7
----------------------------------
592-    State:          Waiting
620-      Reason:       CrashLoopBackOff
657:    Last State:     Terminated
688-      Reason:       Error
714-      Exit Code:    137
----------------------------------
0:Name:         kubia-liveness
29-Namespace:    default
51-Priority:     0
--
--
842-    Ready:          False
868-    Restart Count:  9
890:    Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
986-    Environment:    <none>
1013-    Mounts:
--
--
1593-  Type     Reason     Age                   From               Message
1664-  ----     ------     ----                  ----               -------
1735:  Normal   Scheduled  27m                   default-scheduler  Successfully assigned default/kubia-liveness to minikube
1855-  Normal   Created    23m (x3 over 26m)     kubelet, minikube  Created container kubia
1942-  Normal   Started    23m (x3 over 26m)     kubelet, minikube  Started container kubia
--
--
1855-  Normal   Created    23m (x3 over 26m)     kubelet, minikube  Created container kubia
1942-  Normal   Started    23m (x3 over 26m)     kubelet, minikube  Started container kubia
2029:  Normal   Killing    21m (x3 over 25m)     kubelet, minikube  Container kubia failed liveness probe, will be restarted
2149-  Normal   Pulling    21m (x4 over 27m)     kubelet, minikube  Pulling image "luksa/kubia-unhealthy"
2250-  Normal   Pulled     21m (x4 over 26m)     kubelet, minikube  Successfully pulled image "luksa/kubia-unhealthy"
--
--
2250-  Normal   Pulled     21m (x4 over 26m)     kubelet, minikube  Successfully pulled image "luksa/kubia-unhealthy"
2363-  Warning  BackOff    6m59s (x27 over 14m)  kubelet, minikube  Back-off restarting failed container
2463:  Warning  Unhealthy  85s (x28 over 25m)    kubelet, minikube  Liveness probe failed: HTTP probe failed with statuscode: 500
----------------------------------
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  27m                   default-scheduler  Successfully assigned default/kubia-liveness to minikube
  Normal   Created    23m (x3 over 26m)     kubelet, minikube  Created container kubia
  Normal   Started    23m (x3 over 26m)     kubelet, minikube  Started container kubia
  Normal   Killing    21m (x3 over 25m)     kubelet, minikube  Container kubia failed liveness probe, will be restarted
  Normal   Pulling    21m (x4 over 27m)     kubelet, minikube  Pulling image "luksa/kubia-unhealthy"
  Normal   Pulled     21m (x4 over 26m)     kubelet, minikube  Successfully pulled image "luksa/kubia-unhealthy"
  Warning  BackOff    6m59s (x27 over 14m)  kubelet, minikube  Back-off restarting failed container
  Warning  Unhealthy  85s (x28 over 25m)    kubelet, minikube  Liveness probe failed: HTTP probe failed with statuscode: 500


From the liveness heading above we see that the delay is set to 0s
That means it starts probing as soon as the new container gets created
it is a good idea to add `initialDelaySeconds: 15` so the probe waits for the container to get ready before hitting

Exit code 137 means pod was terminated by external process ie k8s, it failed the livenessProbe and was killed

Make sure liveness only checks things internal to the pod, 
a front end pod's liveness probe should not report failed if the db goes away

Keep the probe light weight, do not use a call that is a lot of compute or time

A pitfall of livenessProbe is Java, do not use exec probe as it will have to spin up jvm every time


* ReplicationControllers
  3 Parts of a replicationControler
- Label selector (seems to be interchangeable with pod selector?)
- Replica count
- Pod template

Lets go create one so we can look at the anatomy

#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/kubia-rc.yaml
  apiVersion: v1
  kind: ReplicationController
  metadata:
    name: kubia
  spec:
    replicas: 3
    selector:
      app: kubia
    template:
      metadata:
        labels:
          app: kubia
      spec:
        containers: 
        - name: kubia
          image: bernokl/kubia
          ports:
            - containerPort: 8080
    #+end_src

Tangled out the yaml with my trusty ,bt it still make sme happy, what can I say
Ok lets go apply the new yaml and see if we get 3 new instances of kubia running
TIP: if you do not define the selector in rc spec it will be derived from pod template
no need to add it.

ok
Its been a while lets see what pods are running in default
#+begin_src shell :results raw
kubectl get pods 
#+end_src

#+RESULTS:
NAME             READY   STATUS             RESTARTS   AGE
kubia-liveness   0/1     CrashLoopBackOff   177        20h

Good so the liveness test is still going, will leave it for now
Lets go apply our yaml and see if we conjured a new rc for kubia
#+begin_src shell :results raw
kubectl create -f yaml/kubia-rc.yaml 
#+end_src

#+RESULTS:
replicationcontroller/kubia created

Lets check what we did
#+begin_src shell :results raw
kubectl get pods 
#+end_src

#+RESULTS:
NAME             READY   STATUS             RESTARTS   AGE
kubia-liveness   0/1     CrashLoopBackOff   179        20h
kubia-pqwfg      1/1     Running            0          15s
kubia-qcjmd      1/1     Running            0          15s
kubia-v5zpr      1/1     Running            0          15s

O nice, I have 3 running pods, lets go off the reservation and kill one of them, see what happens

#+begin_src shell :results raw
kubectl delete po kubia-pqwfg
#+end_src

#+RESULTS:
pod "kubia-pqwfg" deleted


#+begin_src shell :results raw
kubectl get pods 
#+end_src

#+RESULTS:
NAME             READY   STATUS    RESTARTS   AGE
kubia-f6jv5      1/1     Running   0          59s
kubia-liveness   1/1     Running   182        21h
kubia-qcjmd      1/1     Running   0          10m
kubia-v5zpr      1/1     Running   0          10m

O snap, that container got replaced by a new one, go replicationControler go
Lets see what info we can get on it


#+begin_src shell :results raw
kubectl get rc
#+end_src

#+RESULTS:
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       13m

Lets see if we can look at the description

#+begin_src shell :results raw
kubectl describe rc kubia
#+end_src

#+RESULTS:
Name:         kubia
Namespace:    default
Selector:     app=kubia
Labels:       app=kubia
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        bernokl/kubia
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From                    Message
  ----    ------            ----   ----                    -------
  Normal  SuccessfulCreate  14m    replication-controller  Created pod: kubia-pqwfg
  Normal  SuccessfulCreate  14m    replication-controller  Created pod: kubia-qcjmd
  Normal  SuccessfulCreate  14m    replication-controller  Created pod: kubia-v5zpr
  Normal  SuccessfulCreate  4m52s  replication-controller  Created pod: kubia-f6jv5

 
This is cool, we are going to change the labels on pods to show that there is no relationship
between the pod and the rc besides the label, as soon as we change the label of one of our pods
my bet is that a new pod will be created because the kubia label count will be one short
#+begin_src shell :results raw
kubectl get pods
#+end_src

#+RESULTS:
NAME             READY   STATUS             RESTARTS   AGE
kubia-f6jv5      1/1     Running            0          15m
kubia-liveness   0/1     CrashLoopBackOff   185        21h
kubia-qcjmd      1/1     Running            0          24m
kubia-v5zpr      1/1     Running            0          24m


#+begin_src shell :results raw
kubectl label pod kubia-f6jv5 type=special
#+end_src

#+RESULTS:
pod/kubia-f6jv5 labeled


#+begin_src shell :results raw
kubectl get pods --show-labels
#+end_src

#+RESULTS:
NAME             READY   STATUS             RESTARTS   AGE   LABELS
kubia-f6jv5      1/1     Running            0          16m   app=kubia,type=special
kubia-liveness   0/1     CrashLoopBackOff   185        21h   <none>
kubia-qcjmd      1/1     Running            0          25m   app=kubia
kubia-v5zpr      1/1     Running            0          25m   app=kubia

You can see the rc does not care about additional labels

Now lets overwrite app on that on
#+begin_src shell :results raw
kubectl label pod kubia-f6jv5 app=foo --overwrite
#+end_src

#+RESULTS:
pod/kubia-f6jv5 labeled


Lets go see
#+begin_src shell :results raw
kubectl get pods -L app
#+end_src

#+RESULTS:
NAME             READY   STATUS              RESTARTS   AGE   APP
kubia-f6jv5      1/1     Running             0          20m   foo
kubia-liveness   1/1     Running             187        21h   
kubia-qcjmd      1/1     Running             0          29m   kubia
kubia-v5zpr      1/1     Running             0          29m   kubia
kubia-wbpzv      0/1     ContainerCreating   0          3s    kubia

Ahh snap, new container coming up because the count of kubia was one short

k lets clean up a bit going to delete foo and liveness test
#+begin_src shell :results raw
kubectl delete pod kubia-f6jv5
#+end_src

#+RESULTS:
pod "kubia-f6jv5" deleted
pod "kubia-liveness" deleted

O the next plan is to directly edit rc to add an additional label to the pod template,
wonder what will happen here, my bet is we will hang the editor
#+begin_src shell : results raw
kubectl edit rc kubia 
#+end_src

#+RESULTS:

It did not hang, but rather errored out trying to open, vi do I want to set my editor to emacs? mmm lets hold off, gonna go run that in the terminal, back in a jiff
k edited the rc to add a second label to pod template, lets see what I did

#+begin_src shell :results raw
kubectl get pods -L app
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE   APP
kubia-qcjmd   1/1     Running   0          53m   kubia
kubia-v5zpr   1/1     Running   0          53m   kubia
kubia-wbpzv   1/1     Running   0          23m   kubia

k no change, lets delete one

#+begin_src shell :results raw
kubectl delete pod kubia-qcjmd
#+end_src

#+RESULTS:
pod "kubia-qcjmd" deleted


#+begin_src shell :results raw
kubectl get pods --show-labels
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE     LABELS
kubia-98ppm   1/1     Running   0          2m38s   app=kubia,bk=whoot
kubia-v5zpr   1/1     Running   0          57m     app=kubia
kubia-wbpzv   1/1     Running   0          27m     app=kubia

Lets go back to the editor bit, I see there is kube_editor, let me try to set it to emacs, wee what happens
#+begin_src shell :results raw
which emacs 
#+end_src

#+RESULTS:
/usr/local/bin/emacs

#+begin_src shell :results raw
  set KUBE_EDITOR emacs
#+end_src

#+RESULTS:

Lets try and edit again
#+begin_src shell : results raw
kubectl edit rc kubia 
#+end_src

#+RESULTS:

Nope still falls back to vi, there is a really nice kubernetes layer built in so going to leave this for now

Back to ReplicationControllers, lets go add some replicas, see what we seems


#+begin_src shell : results raw
kubectl scale rc kubia --replicas=10 
#+end_src

#+RESULTS:
: replicationcontroller/kubia scaled

#+begin_src shell : results raw
kubectl get pods
#+end_src

#+RESULTS:
| NAME        | READY | STATUS            | RESTARTS | AGE |
| kubia-9482q | 1/1   | Running           |        0 | 23s |
| kubia-98ppm | 1/1   | Running           |        0 | 13m |
| kubia-9xc8x | 0/1   | ContainerCreating |        0 | 23s |
| kubia-cbqmm | 1/1   | Running           |        0 | 23s |
| kubia-nhp94 | 1/1   | Running           |        0 | 23s |
| kubia-pfsdq | 1/1   | Running           |        0 | 23s |
| kubia-v4ss5 | 1/1   | Running           |        0 | 23s |
| kubia-v5zpr | 1/1   | Running           |        0 | 68m |
| kubia-wbpzv | 1/1   | Running           |        0 | 39m |
| kubia-wk77p | 1/1   | Running           |        0 | 23s |

That is badass


#+begin_src shell : results raw
kubectl scale rc kubia --replicas=3 
#+end_src

#+RESULTS:
: replicationcontroller/kubia scaled

#+begin_src shell : results raw
kubectl get pods
#+end_src

#+RESULTS:
| NAME        | READY | STATUS      | RESTARTS | AGE |
| kubia-9482q | 1/1   | Terminating |        0 | 84s |
| kubia-98ppm | 1/1   | Running     |        0 | 14m |
| kubia-9xc8x | 1/1   | Terminating |        0 | 84s |
| kubia-cbqmm | 1/1   | Terminating |        0 | 84s |
| kubia-nhp94 | 1/1   | Terminating |        0 | 84s |
| kubia-pfsdq | 1/1   | Terminating |        0 | 84s |
| kubia-v4ss5 | 1/1   | Terminating |        0 | 84s |
| kubia-v5zpr | 1/1   | Running     |        0 | 69m |
| kubia-wbpzv | 1/1   | Running     |        0 | 40m |
| kubia-wk77p | 1/1   | Terminating |        0 | 84s |

So cool so damn trivial. One very important thing, the scale command, edits rc kubia and changes the `replicas:`
We are not telling k8s to change the pods, we are updating our declared state and k8s adjust reality to fit that state
Neat huh?

When rc's get deleted they typically clean up the pods, but because there is nothing your pods need from the
rc you can delete it without deleteing the pods simply use
#+begin_src shell :results raw
kubectl delete rc kubia --cascade=false
#+end_src

#+RESULTS:

OK, it seems replicationControlers are being replaced with ReplicaSets, you do not directly create them as a rule
they are usually created by higher level processes like deploys. ReplicaSets can do a few things replicationControlers
can not, for instance they can accept multiple labels, they can also use label key without value, so it would allow you
to run env=prod, env=dev etc as labels: env

Lets go manually make a replicaSet


#+begin_src yaml :tangle yaml/kubia-replicaset.yaml
  apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    name: kubia
  spec: 
    replicas: 3
    selector:
      matchLabels:
        app: kubia
    template:
      metadata:
        labels:
          app: kubia
      spec:
        containers:
        - name: kubia
          image: bernokl/kubia
          ports:
            - containerPort: 8080
#+end_src

#+RESULTS:

#+begin_src shell :results raw
kubectl get pods
#+end_src

#+RESULTS:
NAME          READY   STATUS    RESTARTS   AGE
kubia-rf7gz   1/1     Running   0          4d6h
kubia-v5zpr   1/1     Running   0          4d9h
kubia-wbpzv   1/1     Running   0          4d9h
NAME          READY   STATUS    RESTARTS   AGE
kubia-98ppm   1/1     Running   0          111m
kubia-v5zpr   1/1     Running   0          166m
kubia-wbpzv   1/1     Running   0          137m

#+begin_src shell :results raw
kubectl delete -f yaml/kubia-replicaset.yaml
#+end_src

#+RESULTS:
replicaset.apps "kubia" deleted
replicaset.apps/kubia created
replicaset.apps "kubia" deleted
replicaset.apps/kubia created


#+begin_src shell :results raw
  kubectl get replicaSet -A
#+end_src

#+RESULTS:
NAMESPACE              NAME                                  DESIRED   CURRENT   READY   AGE
default                kubia                                 3         3         3       18s
kube-system            coredns-66bff467f8                    1         1         1       89d
kube-system            ingress-nginx-controller-69ccf5d9d8   0         0         0       89d
kube-system            ingress-nginx-controller-799dcd5d57   1         1         1       13d
kubernetes-dashboard   dashboard-metrics-scraper-dc6947fbf   1         1         1       13d
kubernetes-dashboard   kubernetes-dashboard-58b79879c5       1         1         1       13d
NAMESPACE              NAME                                  DESIRED   CURRENT   READY   AGE
default                kubia                                 3         3         3       42m
kube-system            coredns-66bff467f8                    1         1         1       85d
kube-system            ingress-nginx-controller-69ccf5d9d8   0         0         0       85d
kube-system            ingress-nginx-controller-799dcd5d57   1         1         1       9d
kubernetes-dashboard   dashboard-metrics-scraper-dc6947fbf   1         1         1       9d
kubernetes-dashboard   kubernetes-dashboard-58b79879c5       1         1         1       9d

#+begin_src shell :results raw
kubectl describe rs
#+end_src

#+RESULTS:
Name:         kubia
Namespace:    default
Selector:     app=kubia
Labels:       <none>
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        bernokl/kubia
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  40s   replicaset-controller  Created pod: kubia-jbnb6
  Normal  SuccessfulCreate  40s   replicaset-controller  Created pod: kubia-7ljzb
  Normal  SuccessfulCreate  40s   replicaset-controller  Created pod: kubia-74fkq


Biggest improvement for rs over rc is the matchExpressions, it is very powerful

#+begin_src shell
kubectl get pods -n default 
#+end_src

#+RESULTS:
| NAME        | READY | STATUS  | RESTARTS | AGE   |
| kubia-74fkq | 1/1   | Running |        0 | 3m46s |
| kubia-7ljzb | 1/1   | Running |        0 | 3m46s |
| kubia-jbnb6 | 1/1   | Running |        0 | 3m46s |


#+begin_src yaml :tangle yaml/kubia-replicaset-matchexpressions.yaml
  apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    name: kubia
  spec: 
    replicas: 3
    selector:
      matchExpressions:
        - key: app
          operator: In
          values:
            - kubia
    template:
      metadata:
        labels:
          app: kubia
      spec:
        containers:
        - name: kubia
          image: bernokl/kubia
          ports:
            - containerPort: 8080
#+end_src

#+begin_src shell :results raw
kubectl create -f yaml/kubia-replicaset-matchExpressions.yaml 
#+end_src

#+RESULTS:
replicaset.apps/kubia created

#+begin_src shell :results raw
kubectl describe rs  
#+end_src

#+RESULTS:
Name:         kubia
Namespace:    default
Selector:     app in (kubia)
Labels:       <none>
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        bernokl/kubia
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From                   Message
  ----    ------            ----   ----                   -------
  Normal  SuccessfulCreate  3m47s  replicaset-controller  Created pod: kubia-6rts5
  Normal  SuccessfulCreate  3m47s  replicaset-controller  Created pod: kubia-v96d4
  Normal  SuccessfulCreate  3m47s  replicaset-controller  Created pod: kubia-c5mfw
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       3m30s

- In—Label’s value must match one of the specified values.
- NotIn—Label’s value must not match any of the specified values.
- Exists—Pod must include a label with the specified key (the value isn’t import-
ant). When using this operator, you shouldn’t specify the values field.
- DoesNotExist—Pod must not include a label with the specified key. The values
property must not be specified.

Lets clean up
#+begin_src shell :results raw
kubectl delete rs kubia 
#+end_src

#+RESULTS:
replicaset.apps "kubia" deleted

#+begin_src shell :results raw
kubectl get po -A 
#+end_src

#+RESULTS:
NAMESPACE              NAME                                        READY   STATUS      RESTARTS   AGE
kube-system            coredns-66bff467f8-96gv4                    1/1     Running     1          89d
kube-system            etcd-minikube                               1/1     Running     0          13d
kube-system            ingress-nginx-admission-create-fpcvv        0/1     Completed   0          89d
kube-system            ingress-nginx-admission-patch-6qfcz         0/1     Completed   0          89d
kube-system            ingress-nginx-controller-799dcd5d57-cmd68   1/1     Running     0          13d
kube-system            kube-apiserver-minikube                     1/1     Running     1          89d
kube-system            kube-controller-manager-minikube            1/1     Running     1          89d
kube-system            kube-proxy-d892r                            1/1     Running     1          89d
kube-system            kube-scheduler-minikube                     1/1     Running     1          89d
kube-system            registry-65xp4                              1/1     Running     1          89d
kube-system            registry-proxy-dqn2b                        1/1     Running     1          89d
kube-system            storage-provisioner                         1/1     Running     5          89d
kubernetes-dashboard   dashboard-metrics-scraper-dc6947fbf-bp98k   1/1     Running     0          13d
kubernetes-dashboard   kubernetes-dashboard-58b79879c5-2w2wl       1/1     Running     0          13d

* DaemonSet
- This is a resource that is supposed to run one pod on each node
- examples is sytem level resources you want on each node, logging, monitoring etc.
- It is not controlled by the scheduler, will add pods to nodes with scheduling off
- You can add a nodeSelector that will allow daemonset to run on a subset of nodes.

#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/ssd-monitor-daemonset.yaml
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    name: ssd-monitor
  spec:
    selector:
      matchLabels:
        app: ssd-monitor
    template:
      metadata:
        labels:
          app: ssd-monitor
      spec:
        nodeSelector:
          disk: ssd
        containers:
        - name: main
          image: luksa/ssd-monitor
#+end_src


Ok lets apply that sucker
#+begin_src shell :results raw
kubectl apply -f yaml/ssd-monitor-daemonset.yaml 
#+end_src

#+RESULTS:
daemonset.apps/ssd-monitor created

#+begin_src shell :results raw
kubectl get ds
#+end_src

#+RESULTS:
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   0         0         0       0            0           disk=ssd        29s

See no pods running, because none of the nodes have a label disk=ssd, lets look
#+begin_src shell :results raw
kubectl get node 
#+end_src

#+RESULTS:
NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   89d   v1.18.3

Lets go label it, wait before we do that lets jsut confirm the pods we see

#+begin_src shell :results raw
kubectl get pods -A
#+end_src

#+RESULTS:
NAMESPACE              NAME                                        READY   STATUS      RESTARTS   AGE
kube-system            coredns-66bff467f8-96gv4                    1/1     Running     1          89d
kube-system            etcd-minikube                               1/1     Running     0          13d
kube-system            ingress-nginx-admission-create-fpcvv        0/1     Completed   0          89d
kube-system            ingress-nginx-admission-patch-6qfcz         0/1     Completed   0          89d
kube-system            ingress-nginx-controller-799dcd5d57-cmd68   1/1     Running     0          13d
kube-system            kube-apiserver-minikube                     1/1     Running     1          89d
kube-system            kube-controller-manager-minikube            1/1     Running     1          89d
kube-system            kube-proxy-d892r                            1/1     Running     1          89d
kube-system            kube-scheduler-minikube                     1/1     Running     1          89d
kube-system            registry-65xp4                              1/1     Running     1          89d
kube-system            registry-proxy-dqn2b                        1/1     Running     1          89d
kube-system            storage-provisioner                         1/1     Running     5          89d
kubernetes-dashboard   dashboard-metrics-scraper-dc6947fbf-bp98k   1/1     Running     0          13d
kubernetes-dashboard   kubernetes-dashboard-58b79879c5-2w2wl       1/1     Running     0          13d

#+begin_src shell :results raw
kubectl label node minikube disk=ssd 
#+end_src

#+RESULTS:
node/minikube labeled

#+begin_src shell :results raw
kubectl get pods
#+end_src

#+RESULTS:
NAME                READY   STATUS    RESTARTS   AGE
ssd-monitor-vgqwv   1/1     Running   0          6m42s
NAMESPACE              NAME                                        READY   STATUS      RESTARTS   AGE
default                ssd-monitor-vgqwv                           1/1     Running     0          17s
kube-system            coredns-66bff467f8-96gv4                    1/1     Running     1          89d
kube-system            etcd-minikube                               1/1     Running     0          13d
kube-system            ingress-nginx-admission-create-fpcvv        0/1     Completed   0          89d
kube-system            ingress-nginx-admission-patch-6qfcz         0/1     Completed   0          89d
kube-system            ingress-nginx-controller-799dcd5d57-cmd68   1/1     Running     0          13d
kube-system            kube-apiserver-minikube                     1/1     Running     1          89d
kube-system            kube-controller-manager-minikube            1/1     Running     1          89d
kube-system            kube-proxy-d892r                            1/1     Running     1          89d
kube-system            kube-scheduler-minikube                     1/1     Running     1          89d
kube-system            registry-65xp4                              1/1     Running     1          89d
kube-system            registry-proxy-dqn2b                        1/1     Running     1          89d
kube-system            storage-provisioner                         1/1     Running     5          89d
kubernetes-dashboard   dashboard-metrics-scraper-dc6947fbf-bp98k   1/1     Running     0          13d
kubernetes-dashboard   kubernetes-dashboard-58b79879c5-2w2wl       1/1     Running     0          13d


#+begin_src shell :results raw
  kubectl label node minikube disk=hdd --overwrite
#+end_src

#+RESULTS:
node/minikube labeled
node/minikube not labeled


#+begin_src shell :results raw
kubectl get pods
#+end_src

#+RESULTS:
NAME                READY   STATUS        RESTARTS   AGE
ssd-monitor-vgqwv   1/1     Terminating   0          10m
NAME                READY   STATUS    RESTARTS   AGE
ssd-monitor-vgqwv   1/1     Running   0          9m4s
NAME                READY   STATUS    RESTARTS   AGE
ssd-monitor-vgqwv   1/1     Running   0          8m30s

#+begin_src shell :results raw
kubectl get ds 
#+end_src

#+RESULTS:
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   0         0         0       0            0           disk=ssd        16m

#+begin_src shell :results raw
kubectl delete ds ssd-monitor 
#+end_src

#+RESULTS:
daemonset.apps "ssd-monitor" deleted

#+begin_src shell :results raw
kubectl get ds 
#+end_src

#+RESULTS:

* Job
- Used to manage pods that run a task and exits on completion
- Exmple would be ETL job, lots of prow jobs

#+begin_src yaml :tangle /Users/bernokl/projects/k8sInAction/yaml/job-exporter.yaml
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: batch-job
  spec:
    template:
      metadata:
        labels:
          app: batch
      spec:
        restartPolicy: OnFailure
        containers:
        - name: main
          image: luksa/batch-job
#+end_src

#+begin_src shell :results raw
kubectl apply -f yaml/job-exporter.yaml
#+end_src

#+RESULTS:
job.batch/batch-job created

#+begin_src shell :results raw
kubectl get po
#+end_src

#+RESULTS:
NAME              READY   STATUS      RESTARTS   AGE
batch-job-rmf74   0/1     Completed   0          2m9s
NAME              READY   STATUS    RESTARTS   AGE
batch-job-rmf74   1/1     Running   0          2m2s
NAME              READY   STATUS    RESTARTS   AGE
batch-job-rmf74   1/1     Running   0          66s
NAME              READY   STATUS              RESTARTS   AGE
batch-job-rmf74   0/1     ContainerCreating   0          5s

#+begin_src shell :results raw
kubectl logs batch-job-rmf74 
#+end_src

#+RESULTS:
Tue Nov 17 10:45:05 UTC 2020 Batch job starting
Tue Nov 17 10:47:05 UTC 2020 Finished succesfully

Got down to 146 4.5.4 Running multiple pod instances in a Job
